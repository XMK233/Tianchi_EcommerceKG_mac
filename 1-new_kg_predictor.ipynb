{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9e70c4b-b6e8-439c-a11d-d6aa4b065ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "正在加载数据集...\n",
      "训练数据大小: 1242550\n",
      "测试数据大小: 5000\n",
      "正在构建实体和关系映射...\n",
      "实体数量: 249746\n",
      "关系数量: 500\n",
      "正在创建TransE模型...\n",
      "开始训练模型...\n",
      "\n",
      "Epoch 1/5 训练中...\n",
      "  Batch 1214/1214 - Loss: 947.7490 - Progress: 100.0%\n",
      "Epoch 1/5 - Average Loss: 3473.7357\n",
      "\n",
      "Epoch 2/5 训练中...\n",
      "  Batch 1214/1214 - Loss: 1333.0708 - Progress: 100.0%\n",
      "Epoch 2/5 - Average Loss: 2652.7191\n",
      "\n",
      "Epoch 3/5 训练中...\n",
      "  Batch 825/1214 - Loss: 2496.6084 - Progress: 68.0%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7dc8a6a29f30>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xiuminke/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1477, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/xiuminke/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1441, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7dc9f9a6ca00>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xiuminke/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 321\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m任务完成！\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 321\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 312\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m开始训练模型...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 312\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# 预测尾实体\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# 可以通过max_head_entities参数控制处理的头实体个数，None表示处理全部\u001b[39;00m\n\u001b[1;32m    316\u001b[0m predict_tail_entities(model, test_dataset, mapper, device, max_head_entities)\n",
      "Cell \u001b[0;32mIn[16], line 179\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataset, mapper, device)\u001b[0m\n\u001b[1;32m    176\u001b[0m neg_samples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(neg_samples) \u001b[38;5;241m<\u001b[39m NEGATIVE_SAMPLES:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# 随机选择一个实体作为负样本尾实体\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     neg_id \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentity_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# 确保负样本不是正样本\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m neg_id \u001b[38;5;241m!=\u001b[39m t_ids[i]\u001b[38;5;241m.\u001b[39mitem():\n",
      "File \u001b[0;32m/usr/lib/python3.10/random.py:370\u001b[0m, in \u001b[0;36mRandom.randint\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandint\u001b[39m(\u001b[38;5;28mself\u001b[39m, a, b):\n\u001b[1;32m    367\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return random integer in range [a, b], including both end points.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/random.py:325\u001b[0m, in \u001b[0;36mRandom.randrange\u001b[0;34m(self, start, stop, step)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# stop argument supplied.\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     istop \u001b[38;5;241m=\u001b[39m \u001b[43m_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     istop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(stop)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "# 设置随机种子以确保结果可复现\n",
    "def set_seed(seed=123):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# 数据路径\n",
    "TRAIN_FILE_PATH = \"/mnt/d/forCoding_data/Tianchi_EcommerceKG/originalData/OpenBG500/OpenBG500_train.tsv\"\n",
    "TEST_FILE_PATH = \"/mnt/d/forCoding_data/Tianchi_EcommerceKG/originalData/OpenBG500/OpenBG500_test.tsv\"\n",
    "OUTPUT_FILE_PATH = \"./rst.tsv\"\n",
    "\n",
    "# 超参数\n",
    "EMBEDDING_DIM = 200\n",
    "LEARNING_RATE = 0.01\n",
    "MARGIN = 1.0\n",
    "BATCH_SIZE = 1024\n",
    "NEGATIVE_SAMPLES = 5\n",
    "\n",
    "MAX_LINES = None ## 拿多少个样本训练。\n",
    "EPOCHS = 5 # 200 ## 训练几个epoch。\n",
    "max_head_entities = None  # 可以修改为任意整数，如1000表示只处理前1000个\n",
    "\n",
    "# 数据加载类\n",
    "class KnowledgeGraphDataset(Dataset):\n",
    "    def __init__(self, file_path, is_test=False, max_lines=None):\n",
    "        self.is_test = is_test\n",
    "        self.triples = []\n",
    "        \n",
    "        # 读取文件\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            line_count = 0\n",
    "            for line in f:\n",
    "                # 如果设置了最大行数限制且已达到限制，则停止读取\n",
    "                if max_lines is not None and line_count >= max_lines:\n",
    "                    break\n",
    "                \n",
    "                parts = line.strip().split('\\t')\n",
    "                if is_test:\n",
    "                    # 测试文件只有头实体和关系\n",
    "                    if len(parts) >= 2:\n",
    "                        h, r = parts[0], parts[1]\n",
    "                        self.triples.append((h, r, None))\n",
    "                        line_count += 1\n",
    "                else:\n",
    "                    # 训练文件有完整的三元组\n",
    "                    if len(parts) >= 3:\n",
    "                        h, r, t = parts[0], parts[1], parts[2]\n",
    "                        self.triples.append((h, r, t))\n",
    "                        line_count += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.triples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.triples[idx]\n",
    "\n",
    "# 实体和关系映射管理器\n",
    "class EntityRelationMapper:\n",
    "    def __init__(self):\n",
    "        self.entity_to_id = {}\n",
    "        self.id_to_entity = {}\n",
    "        self.relation_to_id = {}\n",
    "        self.id_to_relation = {}\n",
    "        self.entity_count = 0\n",
    "        self.relation_count = 0\n",
    "    \n",
    "    def add_entity(self, entity):\n",
    "        if entity not in self.entity_to_id:\n",
    "            self.entity_to_id[entity] = self.entity_count\n",
    "            self.id_to_entity[self.entity_count] = entity\n",
    "            self.entity_count += 1\n",
    "    \n",
    "    def add_relation(self, relation):\n",
    "        if relation not in self.relation_to_id:\n",
    "            self.relation_to_id[relation] = self.relation_count\n",
    "            self.id_to_relation[self.relation_count] = relation\n",
    "            self.relation_count += 1\n",
    "    \n",
    "    def build_mappings(self, train_dataset, test_dataset):\n",
    "        # 从训练数据构建映射\n",
    "        for h, r, t in train_dataset.triples:\n",
    "            self.add_entity(h)\n",
    "            self.add_entity(t)\n",
    "            self.add_relation(r)\n",
    "        \n",
    "        # 从测试数据构建映射（确保所有实体和关系都被包含）\n",
    "        for h, r, _ in test_dataset.triples:\n",
    "            self.add_entity(h)\n",
    "            self.add_relation(r)\n",
    "\n",
    "# TransE模型实现\n",
    "class TransE(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim):\n",
    "        super(TransE, self).__init__()\n",
    "        # 初始化实体和关系嵌入\n",
    "        self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)\n",
    "        self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)\n",
    "        \n",
    "        # 初始化嵌入向量\n",
    "        nn.init.xavier_uniform_(self.entity_embeddings.weight.data)\n",
    "        nn.init.xavier_uniform_(self.relation_embeddings.weight.data)\n",
    "        \n",
    "        # 归一化实体嵌入\n",
    "        self.normalize_entities()\n",
    "    \n",
    "    def normalize_entities(self):\n",
    "        # 对实体嵌入进行L2归一化\n",
    "        with torch.no_grad():\n",
    "            norms = torch.norm(self.entity_embeddings.weight, p=2, dim=1, keepdim=True)\n",
    "            self.entity_embeddings.weight.data = self.entity_embeddings.weight.data / norms\n",
    "    \n",
    "    def forward(self, h, r, t, t_neg=None):\n",
    "        # 获取嵌入向量\n",
    "        h_emb = self.entity_embeddings(h)\n",
    "        r_emb = self.relation_embeddings(r)\n",
    "        t_emb = self.entity_embeddings(t)\n",
    "        \n",
    "        # 计算得分：||h + r - t||\n",
    "        pos_score = torch.norm(h_emb + r_emb - t_emb, p=1, dim=1)\n",
    "        \n",
    "        if t_neg is not None:\n",
    "            # 计算负样本得分\n",
    "            t_neg_emb = self.entity_embeddings(t_neg)\n",
    "            h_emb_expanded = h_emb.unsqueeze(1).expand(-1, NEGATIVE_SAMPLES, -1)\n",
    "            r_emb_expanded = r_emb.unsqueeze(1).expand(-1, NEGATIVE_SAMPLES, -1)\n",
    "            \n",
    "            neg_score = torch.norm(h_emb_expanded + r_emb_expanded - t_neg_emb, p=1, dim=2)\n",
    "            return pos_score, neg_score\n",
    "        \n",
    "        return pos_score\n",
    "\n",
    "# 训练函数\n",
    "def train_model(model, train_dataset, mapper, device):\n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    \n",
    "    # 定义优化器和损失函数\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        # 为每个epoch打印进度条\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS} 训练中...\")\n",
    "        \n",
    "        # 使用enumerate来获取batch索引\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            h_batch, r_batch, t_batch = batch\n",
    "            \n",
    "            # 将实体和关系转换为ID\n",
    "            h_ids = torch.tensor([mapper.entity_to_id[h] for h in h_batch], device=device)\n",
    "            r_ids = torch.tensor([mapper.relation_to_id[r] for r in r_batch], device=device)\n",
    "            t_ids = torch.tensor([mapper.entity_to_id[t] for t in t_batch], device=device)\n",
    "            \n",
    "            # 生成负样本\n",
    "            batch_size = h_ids.size(0)\n",
    "            t_neg_ids = []\n",
    "            for i in range(batch_size):\n",
    "                # 为每个正样本生成多个负样本\n",
    "                neg_samples = []\n",
    "                while len(neg_samples) < NEGATIVE_SAMPLES:\n",
    "                    # 随机选择一个实体作为负样本尾实体\n",
    "                    neg_id = random.randint(0, mapper.entity_count - 1)\n",
    "                    # 确保负样本不是正样本\n",
    "                    if neg_id != t_ids[i].item():\n",
    "                        neg_samples.append(neg_id)\n",
    "                t_neg_ids.append(neg_samples)\n",
    "            \n",
    "            t_neg_ids = torch.tensor(t_neg_ids, device=device)\n",
    "            \n",
    "            # 前向传播\n",
    "            pos_score, neg_score = model(h_ids, r_ids, t_ids, t_neg_ids)\n",
    "            \n",
    "            # 计算损失（基于margin的max-margin损失）\n",
    "            # 将pos_score扩展为与neg_score相同的维度\n",
    "            pos_score_expanded = pos_score.unsqueeze(1).expand_as(neg_score)\n",
    "            loss = torch.sum(torch.relu(pos_score_expanded - neg_score + MARGIN))\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 归一化实体嵌入\n",
    "            model.normalize_entities()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # 每个batch打印一次进度\n",
    "            progress = (batch_idx + 1) / len(train_loader) * 100\n",
    "            print(f\"  Batch {batch_idx+1}/{len(train_loader)} - Loss: {loss.item():.4f} - Progress: {progress:.1f}%\", end='\\r')\n",
    "        \n",
    "        # 每个epoch结束后打印平均损失\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS} - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 预测函数 - 优化版本（向量化计算）\n",
    "def predict_tail_entities(model, test_dataset, mapper, device, max_head_entities=None, batch_size=128):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    total_test_triples = len(test_dataset.triples)\n",
    "    \n",
    "    # 确定要处理的头实体数量\n",
    "    process_count = total_test_triples if max_head_entities is None else min(max_head_entities, total_test_triples)\n",
    "    print(f\"\\n开始预测尾实体...\")\n",
    "    print(f\"将处理 {process_count} 个头实体/关系对\")\n",
    "    \n",
    "    # 预计算所有实体嵌入（只需一次，避免重复计算）\n",
    "    all_entities = torch.arange(mapper.entity_count, device=device)\n",
    "    all_entity_embeddings = model.entity_embeddings(all_entities)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 创建批次进行处理，大幅提高GPU利用率\n",
    "        for batch_start in range(0, process_count, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, process_count)\n",
    "            batch_triples = test_dataset.triples[batch_start:batch_end]\n",
    "            \n",
    "            # 获取批次中的头实体和关系\n",
    "            h_list = [h for h, _, _ in batch_triples]\n",
    "            r_list = [r for _, r, _ in batch_triples]\n",
    "            \n",
    "            # 转换为ID和张量\n",
    "            h_ids = torch.tensor([mapper.entity_to_id[h] for h in h_list], device=device)\n",
    "            r_ids = torch.tensor([mapper.relation_to_id[r] for r in r_list], device=device)\n",
    "            \n",
    "            # 获取嵌入向量\n",
    "            h_emb = model.entity_embeddings(h_ids)  # [batch_size, embedding_dim]\n",
    "            r_emb = model.relation_embeddings(r_ids)  # [batch_size, embedding_dim]\n",
    "            \n",
    "            # 计算h + r\n",
    "            h_plus_r = h_emb + r_emb  # [batch_size, embedding_dim]\n",
    "            \n",
    "            # 使用广播机制一次性计算所有可能的尾实体得分\n",
    "            # 这是向量化操作的关键部分，避免了循环遍历每个尾实体\n",
    "            h_plus_r_expanded = h_plus_r.unsqueeze(1)  # [batch_size, 1, embedding_dim]\n",
    "            scores = torch.norm(h_plus_r_expanded - all_entity_embeddings.unsqueeze(0), p=1, dim=2)  # [batch_size, entity_count]\n",
    "            \n",
    "            # 获取每个头实体/关系对的top10尾实体\n",
    "            _, top10_indices = torch.topk(-scores, k=10, dim=1)  # 使用负数以获取最小的10个值\n",
    "            \n",
    "            # 处理批次结果\n",
    "            for i in range(len(batch_triples)):\n",
    "                h, r, _ = batch_triples[i]  # 正确解包3元素元组\n",
    "                top10_t_ids = top10_indices[i].tolist()\n",
    "                \n",
    "                # 转换回实体ID字符串\n",
    "                top10_entities = [mapper.id_to_entity[t_id] for t_id in top10_t_ids]\n",
    "                \n",
    "                # 构建结果行\n",
    "                result_line = [h, r] + top10_entities\n",
    "                results.append('\\t'.join(result_line))\n",
    "            \n",
    "            # 显示进度\n",
    "            progress = batch_end / process_count * 100\n",
    "            print(f\"  预测进度: {batch_end}/{process_count} - {progress:.1f}%\", end='\\r')\n",
    "    \n",
    "    print()  # 确保进度条完成后换行\n",
    "    \n",
    "    # 写入结果文件\n",
    "    with open(OUTPUT_FILE_PATH, 'w', encoding='utf-8') as f:\n",
    "        for line in results:\n",
    "            f.write(line + '\\n')\n",
    "    \n",
    "    print(f\"预测结果已保存到 {OUTPUT_FILE_PATH}\")\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    # 检查CUDA是否可用\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"使用设备: {device}\")\n",
    "    \n",
    "    # 加载数据集\n",
    "    print(\"正在加载数据集...\")\n",
    "    # 限制训练数据为10000行以节省训练时间\n",
    "    train_dataset = KnowledgeGraphDataset(TRAIN_FILE_PATH, max_lines=MAX_LINES)\n",
    "    test_dataset = KnowledgeGraphDataset(TEST_FILE_PATH, is_test=True)\n",
    "    \n",
    "    print(f\"训练数据大小: {len(train_dataset)}\")\n",
    "    print(f\"测试数据大小: {len(test_dataset)}\")\n",
    "    \n",
    "    # 构建实体和关系映射\n",
    "    print(\"正在构建实体和关系映射...\")\n",
    "    mapper = EntityRelationMapper()\n",
    "    mapper.build_mappings(train_dataset, test_dataset)\n",
    "    \n",
    "    print(f\"实体数量: {mapper.entity_count}\")\n",
    "    print(f\"关系数量: {mapper.relation_count}\")\n",
    "    \n",
    "    # 创建模型\n",
    "    print(\"正在创建TransE模型...\")\n",
    "    model = TransE(mapper.entity_count, mapper.relation_count, EMBEDDING_DIM)\n",
    "    model.to(device)\n",
    "    \n",
    "    # 训练模型\n",
    "    print(\"开始训练模型...\")\n",
    "    train_model(model, train_dataset, mapper, device)\n",
    "    \n",
    "    # 预测尾实体\n",
    "    # 可以通过max_head_entities参数控制处理的头实体个数，None表示处理全部\n",
    "    predict_tail_entities(model, test_dataset, mapper, device, max_head_entities)\n",
    "    \n",
    "    print(\"任务完成！\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cf3431-1a25-4116-9550-57044f4935b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce1977a-150b-47be-91f6-fa2a008869bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74508078-fff9-466d-a83f-48ba46a1f127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a442b0a4-e167-4f2f-ad41-47eca6f719c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ecdb1e-d12e-4cf8-97df-a52e08b80068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c08e27-09b8-49ce-87ae-8ed28cd2eaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9408f37e-e2c9-45fa-8be9-cc7aa7d87f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff07a2-22ad-4d64-803f-3784fab1bbd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72641830-76d1-4ca5-bd88-eeeaa902724a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b4407-2862-4736-8498-082232eed411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99917dd4-a7be-4e3e-806c-02fd35778db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ef9474-fd8e-45ab-a4cc-325760fb8359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75eb0ebd-b587-408b-867f-d1bcd99874ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage dir: /mnt/d/forCoding_data/Tianchi_EcommerceKG\n",
      "code dir: /mnt/d/forCoding_code/Tianchi_EcommerceKG\n"
     ]
    }
   ],
   "source": [
    "import random, os, tqdm, time, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "random.seed(618)\n",
    "np.random.seed(907)\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "## mac环境\n",
    "# new_base_path = os.path.join(\n",
    "#     \"/Users/minkexiu/Downloads/\",\n",
    "#     \"/\".join(\n",
    "#         os.getcwd().split(\"/\")[-1*(len(sys.path[-1].split(\"/\")) - 1):]\n",
    "#     ),\n",
    "# )\n",
    "## WSL环境\n",
    "new_base_path = os.path.join(\n",
    "    \"/mnt/d/forCoding_data/\",\n",
    "    \"/\".join(\n",
    "        os.getcwd().split(\"/\")[-1*(len(sys.path[-1].split(\"/\")) - 1):]\n",
    "    ),\n",
    ")\n",
    "## wWindows环境\n",
    "# sys.path.append(\"..\\\\..\\\\\")\n",
    "# new_base_path = os.path.join(\n",
    "#     \"D:\\\\forCoding_data\\\\\",\n",
    "#     \"\\\\\".join(\n",
    "#         os.getcwd().split(\"\\\\\")[-1*(len(sys.path[-1].split(\"\\\\\")) - 1):]\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "print(\"storage dir:\", new_base_path)\n",
    "print(\"code dir:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d69dbf59-ac68-4c9e-a0c7-cc407a42e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 创建文件夹。\n",
    "if not os.path.exists(new_base_path):\n",
    "    os.makedirs(\n",
    "        new_base_path\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"preprocessedData\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"preprocessedData\")\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"originalData\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"originalData\")\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"trained_models\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"trained_models\")\n",
    "    )\n",
    "\n",
    "def create_originalData_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"originalData\", filename_or_path)\n",
    "def create_preprocessedData_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"preprocessedData\", filename_or_path)\n",
    "def create_trained_models_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"trained_models\", filename_or_path)\n",
    "\n",
    "def millisec2datetime(timestamp):\n",
    "    time_local = time.localtime(timestamp/1000)\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time_local)\n",
    "    \n",
    "def run_finish():\n",
    "    # 假设你的字体文件是 'myfont.ttf' 并且位于当前目录下  \n",
    "    font = FontProperties(fname=\"/mnt/d/SimHei.ttf\", size=24)  \n",
    "    # 创建一个空白的图形  \n",
    "    fig, ax = plt.subplots()  \n",
    "    ax.imshow(\n",
    "        \n",
    "        plt.imread(\"/mnt/c/Users/Administrator/Pictures/Saved Pictures/ComfyUI_00044_.png\")\n",
    "    )\n",
    "    # 在图形中添加文字  \n",
    "    ax.text(\n",
    "        ax.get_xlim()[1] * 0.5, \n",
    "        ax.get_ylim()[0] * 0.5, \n",
    "        f\"程序于这个点跑完：\\n{millisec2datetime(time.time()*1000)}\", fontproperties=font, ha=\"center\", va=\"center\", color=\"red\"\n",
    "    )  \n",
    "    # 设置图形的布局  \n",
    "    # ax.set_xlim(0, 1)  \n",
    "    # ax.set_ylim(0, 1)  \n",
    "    ax.set_xticks([])  \n",
    "    ax.set_yticks([])  \n",
    "    ax.patch.set_color(\"blue\")\n",
    "    # 显示图形  \n",
    "    plt.show()\n",
    "        \n",
    "tqdm.tqdm.pandas() ## 引入这个，就可以在apply的时候用progress_apply了。\n",
    "\n",
    "import IPython\n",
    "def kill_current_kernel():\n",
    "    '''杀死当前的kernel释放内存空间。'''\n",
    "    IPython.Application.instance().kernel.do_shutdown(True) \n",
    "    \n",
    "def simply_show_data(df1):\n",
    "    print(df1.shape)\n",
    "    display(df1.head())\n",
    "    \n",
    "def wait_flag(saved_flag_path, time_interval_sec=10):\n",
    "    print(\"waiting for\", saved_flag_path)\n",
    "    time_count = 0\n",
    "    while True:\n",
    "        if os.path.exists(saved_flag_path):\n",
    "            break\n",
    "        time.sleep(time_interval_sec)\n",
    "        time_count+=time_interval_sec\n",
    "        print(time_count, end=\" \")\n",
    "    print(\"finish!!\")\n",
    "\n",
    "class TimerContext:  \n",
    "    def __enter__(self):  \n",
    "        self.start_time = str(datetime.now())\n",
    "        print(\"start time:\", self.start_time)\n",
    "        return self  \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):  \n",
    "        print(\"start time:\", self.start_time)\n",
    "        print(\"end time\", str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cdeb6a3-fa0f-487d-908d-c3a99deafdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/forCoding_data/Tianchi_EcommerceKG/originalData/OpenBG500/OpenBG500_train.tsv\n"
     ]
    }
   ],
   "source": [
    "pth1 = create_originalData_path(\"OpenBG500/OpenBG500_train.tsv\")\n",
    "print(pth1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfe5e227-3777-4561-948a-3f580ea7e0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ent_135492', 'rel_0352', 'ent_015651']\n",
      "['ent_020765', 'rel_0448', 'ent_214183']\n"
     ]
    }
   ],
   "source": [
    "with open(pth1, 'r') as fp:\n",
    "    data = fp.readlines()\n",
    "    train = [line.strip('\\n').split('\\t') for line in data]\n",
    "    _ = [print(line) for line in train[:2]]\n",
    "    # ['ent_135492', 'rel_0352', 'ent_015651']\n",
    "    # ['ent_020765', 'rel_0448', 'ent_214183']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126ab1b5-0a84-48fa-a960-546d50772fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574646b-c8ed-4a58-ae87-d0de88594cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b03be1-71c0-4c7c-b7b7-221ba11288ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
