import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import random
import numpy as np
import os
from tqdm import tqdm
import zipfile
import faiss  # åŠ é€Ÿ Top-K æœç´¢

# è®¾ç½®éšæœºç§å­
torch.backends.cudnn.deterministic = True
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)

scheme_type = "es_fs_e10"

# æ•°æ®è·¯å¾„ï¼ˆè¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹ï¼‰
BASE_DIR = "/Users/minkexiu/Downloads/GitHub/Tianchi_EcommerceKG_mac"
TRAIN_FILE_PATH = f"{BASE_DIR}/originalData/OpenBG500/OpenBG500_train.tsv"
TEST_FILE_PATH = f"{BASE_DIR}/originalData/OpenBG500/OpenBG500_test.tsv"
DEV_FILE_PATH = f"{BASE_DIR}/originalData/OpenBG500/OpenBG500_dev.tsv"
OUTPUT_FILE_PATH = f"{BASE_DIR}/preprocessedData/OpenBG500_test.tsv" 

# æ¨¡å‹ä¿å­˜è·¯å¾„
MODEL_DIR = f"{BASE_DIR}/trained_models/{scheme_type}"
os.makedirs(MODEL_DIR, exist_ok=True)

# æ¨¡å‹è·¯å¾„ 
TRAINED_MODEL_PATHS = {
    'TransE': f"{MODEL_DIR}/transE.pth",
    'TransH': f"{MODEL_DIR}/transH.pth",
    'TransD': f"{MODEL_DIR}/transD.pth"
}

# è¶…å‚æ•°
EMBEDDING_DIM = 100
LEARNING_RATE = 0.001
WEIGHT_DECAY = 1e-5
EPOCHS = 10 ##ã€TODOã€‘è¿™é‡Œå¯ä»¥ä¿®æ”¹å¤šä¸€ç‚¹ã€‚
BATCH_SIZE = 256
NEGATIVE_SAMPLES = 10
MAX_LINES = None
MAX_HEAD_ENTITIES = None
LR_DECAY_STEP = 5
LR_DECAY_FACTOR = 0.1


# ==================== æ•°æ®é›† ====================
class KnowledgeGraphDataset(torch.utils.data.Dataset):
    def __init__(self, file_path, is_test=False, max_lines=None, is_train=False):
        self.triples = []
        self.is_train = is_train
        self._load_data(file_path, is_test, max_lines)

    def _load_data(self, file_path, is_test, max_lines):
        print(f"åŠ è½½æ•°æ®: {file_path}")
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            if max_lines:
                lines = lines[:max_lines]
            for line in lines:
                parts = line.strip().split()
                if len(parts) == 3:
                    h, r, t = parts
                    self.triples.append((h, r, t))
                elif is_test and len(parts) == 2:
                    h, r = parts
                    self.triples.append((h, r, "<UNK>"))
        print(f"å…±åŠ è½½ {len(self.triples)} ä¸ªä¸‰å…ƒç»„")

    def __len__(self):
        return len(self.triples)

    def __getitem__(self, idx):
        return self.triples[idx]


def collate_fn(batch):
    h_list, r_list, t_list = zip(*batch)
    return list(h_list), list(r_list), list(t_list)


# ==================== æ˜ å°„å™¨ ====================
class EntityRelationMapper:
    def __init__(self):
        self.entity_to_id = {}
        self.id_to_entity = {}
        self.relation_to_id = {}
        self.id_to_relation = {}
        self.entity_count = 0
        self.relation_count = 0
        self.all_train_triples = []

    def build_mappings(self, *datasets):
        entities = set()
        relations = set()
        for dataset in datasets:
            for h, r, t in dataset.triples:
                entities.add(h)
                entities.add(t)
                relations.add(r)
                if dataset.is_train:
                    self.all_train_triples.append((h, r, t))

        for e in sorted(entities):
            self.entity_to_id[e] = self.entity_count
            self.id_to_entity[self.entity_count] = e
            self.entity_count += 1
        for r in sorted(relations):
            self.relation_to_id[r] = self.relation_count
            self.id_to_relation[self.relation_count] = r
            self.relation_count += 1


# ==================== TransE ====================
class TransE(nn.Module):
    def __init__(self, num_entities, num_relations, dim):
        super().__init__()
        self.E = nn.Embedding(num_entities, dim)
        self.R = nn.Embedding(num_relations, dim)
        nn.init.xavier_uniform_(self.E.weight)
        nn.init.xavier_uniform_(self.R.weight)

    def forward(self, h, r, t):
        return torch.norm(self.E(h) + self.R(r) - self.E(t), p=1, dim=1)

    def get_query_embedding(self, h, r):
        return self.E(h) + self.R(r)


# ==================== TransH ï¼ˆå·²ä¿®å¤ï¼‰====================
class TransH(nn.Module):
    def __init__(self, num_entities, num_relations, dim):
        super().__init__()
        self.E = nn.Embedding(num_entities, dim)
        self.R = nn.Embedding(num_relations, dim)      # å…³ç³»å‘é‡ d_r
        self.W = nn.Embedding(num_relations, dim)    # æ³•å‘é‡ W (ç”¨äºè¶…å¹³é¢)
        nn.init.xavier_uniform_(self.E.weight)
        nn.init.xavier_uniform_(self.R.weight)
        nn.init.xavier_uniform_(self.W.weight)

    def project(self, emb, w):  # å°†å®ä½“æŠ•å½±åˆ°å…³ç³»è¶…å¹³é¢ä¸Š
        # w: [B, dim], emb: [B, dim]
        norm_w = torch.nn.functional.normalize(w, p=2, dim=1)  # å•ä½åŒ–æ³•å‘é‡
        scale = torch.sum(emb * norm_w, dim=1, keepdim=True)  # <e, w>
        return emb - scale * norm_w  # e - <e, w> * w

    def forward(self, h, r, t):
        h_emb = self.E(h)  # [B, dim]
        t_emb = self.E(t)
        r_vec = self.R(r)
        W = self.W(r)

        h_proj = self.project(h_emb, W)
        t_proj = self.project(t_emb, W)

        return torch.norm(h_proj + r_vec - t_proj, p=1, dim=1)

    def get_query_embedding(self, h, r):
        h_emb = self.E(h)
        r_vec = self.R(r)
        W = self.W(r)
        h_proj = self.project(h_emb, W)
        return h_proj + r_vec  # æŸ¥è¯¢å‘é‡


# ==================== TransD ====================
class TransD(nn.Module):
    def __init__(self, num_entities, num_relations, dim):
        super().__init__()
        self.dim = dim
        self.E = nn.Embedding(num_entities, dim)
        self.R = nn.Embedding(num_relations, dim)
        self.E_proj = nn.Embedding(num_entities, dim)
        self.R_proj = nn.Embedding(num_relations, dim)
        nn.init.xavier_uniform_(self.E.weight)
        nn.init.xavier_uniform_(self.R.weight)
        nn.init.xavier_uniform_(self.E_proj.weight)
        nn.init.xavier_uniform_(self.R_proj.weight)

    def project(self, e, r_proj):
        return e + torch.sum(e * r_proj, dim=1, keepdim=True)

    def forward(self, h, r, t):
        h_emb = self.project(self.E(h), self.R_proj(r))
        t_emb = self.project(self.E(t), self.R_proj(r))
        r_vec = self.R(r)
        return torch.norm(h_emb + r_vec - t_emb, p=1, dim=1)

    def get_query_embedding(self, h, r):
        h_emb = self.project(self.E(h), self.R_proj(r))
        r_vec = self.R(r)
        return h_emb + r_vec


# ==================== è®­ç»ƒ & åŠ è½½ ====================
def train_model(model, model_name, train_dataset, mapper, device):
    if os.path.exists(TRAINED_MODEL_PATHS[model_name]):
        print(f"[{model_name}] å·²å­˜åœ¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè·³è¿‡è®­ç»ƒ")
        return
    print(f"[{model_name}] å¼€å§‹è®­ç»ƒ...")
    loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_DECAY_STEP, gamma=LR_DECAY_FACTOR)
    model.to(device)
    model.train()

    for epoch in range(EPOCHS):
        epoch_loss = 0
        progress = tqdm(loader, desc=f"{model_name} Epoch {epoch+1}")
        for h_list, r_list, t_list in progress:
            h = torch.tensor([mapper.entity_to_id[h] for h in h_list], device=device)
            r = torch.tensor([mapper.relation_to_id[r] for r in r_list], device=device)
            t = torch.tensor([mapper.entity_to_id[t] for t in t_list], device=device)
            ## ã€TODOã€‘è¿™é‡Œè¦ç¡®ä¿è´Ÿæ ·æœ¬å’ŒåŸæ¥çš„ä¸ä¸€æ ·ã€‚
            neg_t = torch.randint(0, mapper.entity_count, (len(h), NEGATIVE_SAMPLES), device=device)             

            pos_score = model(h, r, t)
            neg_score = model(
                h.unsqueeze(1).expand(-1, NEGATIVE_SAMPLES).reshape(-1),
                r.unsqueeze(1).expand(-1, NEGATIVE_SAMPLES).reshape(-1),
                neg_t.reshape(-1)
            ).reshape(-1, NEGATIVE_SAMPLES)

            loss = torch.mean(torch.relu(pos_score.unsqueeze(1) - neg_score + 1.0))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
            progress.set_postfix(loss=loss.item())
        scheduler.step()
        print(f"[{model_name}] Epoch {epoch+1} Loss: {epoch_loss / len(loader):.4f}")

    torch.save({
        'model_state_dict': model.state_dict(),
        'entity_count': mapper.entity_count,
        'relation_count': mapper.relation_count,
        'embedding_dim': EMBEDDING_DIM,
        'entity_to_id': mapper.entity_to_id,
        'relation_to_id': mapper.relation_to_id,
    }, TRAINED_MODEL_PATHS[model_name])
    print(f"[{model_name}] æ¨¡å‹å·²ä¿å­˜")


def load_model(model_class, model_path, mapper, device):
    checkpoint = torch.load(model_path, map_location=device)
    model = model_class(checkpoint['entity_count'], checkpoint['relation_count'], EMBEDDING_DIM)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    return model

def evaluate_model(model, dataset, mapper, device, k_list=(1, 3, 10)):
    print("ğŸ” å¼€å§‹åœ¨å¼€å‘é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ˆä»…å°¾å®ä½“é¢„æµ‹ï¼‰...")
    model.eval()
    hits_at = {k: 0.0 for k in k_list}
    mrr = 0.0
    count = 0

    entity_emb = model.E.weight.data.cpu().numpy()
    index = faiss.IndexFlatL2(entity_emb.shape[1])
    index.add(entity_emb)

    with torch.no_grad():
        for h, r, t in tqdm(dataset.triples, desc="Evaluating"):
            try:
                h_id = torch.tensor([mapper.entity_to_id[h]], device=device)
                r_id = torch.tensor([mapper.relation_to_id[r]], device=device)
                t_id = mapper.entity_to_id[t]
            except KeyError:
                continue

            # åªåšå°¾å®ä½“é¢„æµ‹ (h, r, ?)
            query = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()
            _, indices = index.search(query, 1000)
            pred_ids = indices[0]

            # è¿‡æ»¤
            filtered_tails = [tail for head, rel, tail in mapper.all_train_triples if head == h and rel == r and tail != t]
            filter_ids = [mapper.entity_to_id[tail] for tail in filtered_tails if tail in mapper.entity_to_id]
            for fid in filter_ids:
                if fid in pred_ids:
                    mask = pred_ids == fid
                    pred_ids = np.concatenate([pred_ids[~mask], pred_ids[mask]])

            rank = np.where(pred_ids == t_id)[0]
            final_rank = rank[0] + 1 if len(rank) > 0 else 10000

            for k in k_list:
                if final_rank <= k:
                    hits_at[k] += 1
            mrr += 1.0 / final_rank
            count += 1

    for k in hits_at:
        hits_at[k] /= count
    mrr /= count

    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“Š HITS@1:  {hits_at[1]:.4f}")
    print(f"ğŸ“Š HITS@3:  {hits_at[3]:.4f}")
    print(f"ğŸ“Š HITS@10: {hits_at[10]:.4f}")
    print(f"ğŸ“Š MRR:     {mrr:.4f}")

    return hits_at, mrr

def evaluate_ensemble(models_with_weights, dev_dataset, mapper, device, k_list=(1, 3, 10), rrf_k=60):
# def evaluate_ensemble_rrf(models_with_weights, dev_dataset, mapper, device, k_list=(1, 3, 10), rrf_k=60):
    """
    ä½¿ç”¨ Reciprocal Rank Fusion (RRF) èåˆå¤šä¸ªæ¨¡å‹çš„æ’åºç»“æœ
    ç‰¹åˆ«è¦æ±‚ï¼šè‹¥æ­£ç¡®ç­”æ¡ˆä¸åœ¨ Top10 å€™é€‰ä¸­ï¼Œåˆ™ MRR å¾—åˆ†ä¸º 0
    """
    print("\n" + "="*50)
    print("ğŸš€ å¼€å§‹è¯„ä¼°èåˆæ¨¡å‹ (RRF èåˆ) - Top10 å¤–ç­”æ¡ˆå¾—åˆ†ä¸º 0")
    print("="*50)

    hits_at = {k: 0.0 for k in k_list}
    mrr_scores = []  # æ¯ä¸ª query çš„ MRR å¾—åˆ†ï¼ˆå¯èƒ½ä¸º 0ï¼‰
    count = 0

    # è·å–å®ä½“åµŒå…¥ï¼ˆç”¨äº FAISS æ£€ç´¢ï¼‰
    sample_model = next(iter(models_with_weights.values()))[0]
    entity_emb = sample_model.E.weight.data.cpu().numpy()
    index = faiss.IndexFlatL2(entity_emb.shape[1])
    index.add(entity_emb)

    with torch.no_grad():
        for h, r, t in tqdm(dev_dataset.triples, desc="RRF Eval"):
            try:
                h_id = torch.tensor([mapper.entity_to_id[h]], device=device)
                r_id = torch.tensor([mapper.relation_to_id[r]], device=device)
                t_id = mapper.entity_to_id[t]
            except KeyError:
                # è·³è¿‡æœªç™»å½•å®ä½“
                continue

            # ========== æ”¶é›†æ¯ä¸ªæ¨¡å‹çš„æ’åºå¾—åˆ†ï¼ˆRRFï¼‰==========
            rrf_scores = np.zeros(mapper.entity_count)

            for name, (model, weight) in models_with_weights.items():
                q = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()
                _, indices = index.search(q, 1000)  # æ£€ç´¢ top 1000
                candidate_ids = indices[0]

                # RRF å…¬å¼: score += weight / (k + rank)
                for rank, idx in enumerate(candidate_ids):
                    rrf_scores[idx] += weight / (rrf_k + rank + 1)

            # ========== è¿‡æ»¤è®­ç»ƒé›†ä¸­å·²å­˜åœ¨çš„ä¸‰å…ƒç»„ï¼ˆé™¤å½“å‰å¤–ï¼‰==========
            filtered_tails = [
                tail for head, rel, tail in mapper.all_train_triples
                if head == h and rel == r and tail != t
            ]
            for tail in filtered_tails:
                if tail in mapper.entity_to_id:
                    rrf_scores[mapper.entity_to_id[tail]] = -1e9

            # ========== æ’åºï¼ˆé™åºï¼‰==========
            ranked_indices = np.argsort(rrf_scores)[::-1]

            # ========== è·å– Top10 é¢„æµ‹ç»“æœ ==========
            top10_ids = ranked_indices[:10]
            top10_entities = [mapper.id_to_entity[i] for i in top10_ids]

            # ========== è®¡ç®—æŒ‡æ ‡ ==========
            # æ£€æŸ¥çœŸå® tail æ˜¯å¦åœ¨ Top10
            if t_id in top10_ids:
                rank = np.where(ranked_indices == t_id)[0][0] + 1  # æ’åä» 1 å¼€å§‹
                mrr_score = 1.0 / rank
            else:
                mrr_score = 0.0  # Top10 ä¸åŒ…å«ï¼Œå¾—åˆ†ä¸º 0

            # æ›´æ–° Hits
            for k in k_list:
                if t_id in top10_ids[:k]:
                    hits_at[k] += 1

            mrr_scores.append(mrr_score)
            count += 1

    # ========== è®¡ç®—æœ€ç»ˆæŒ‡æ ‡ ==========
    for k in hits_at:
        hits_at[k] /= count
    mrr = np.mean(mrr_scores) if mrr_scores else 0.0

    print("âœ… RRF èåˆè¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“Š RRF HITS@1:  {hits_at[1]:.4f}")
    print(f"ğŸ“Š RRF HITS@3:  {hits_at[3]:.4f}")
    print(f"ğŸ“Š RRF HITS@10: {hits_at[10]:.4f}")
    print(f"ğŸ“Š RRF MRR:     {mrr:.4f}")

    return hits_at, mrr


# ==================== èåˆé¢„æµ‹ ====================
def predict_ensemble(models_with_weights, test_dataset, mapper, device, max_head_entities=None, rrf_k=60): 
# def predict_ensemble_rrf(models_with_weights, test_dataset, mapper, device, max_head_entities=None, rrf_k=60):
    """
    ä½¿ç”¨ RRF èåˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼ˆä¸ evaluate_ensemble_rrf ä¿æŒä¸€è‡´ï¼‰
    è¾“å‡ºæ¯ä¸ª query çš„ Top10 é¢„æµ‹ç»“æœ
    """
    print("ğŸ” å¼€å§‹èåˆé¢„æµ‹ (RRF èåˆ + FAISS åŠ é€Ÿ) ...")
    results = []

    # è·å–å®ä½“åµŒå…¥
    sample_model = next(iter(models_with_weights.values()))[0]
    entity_emb = sample_model.E.weight.data.cpu().numpy()
    index = faiss.IndexFlatL2(entity_emb.shape[1])
    index.add(entity_emb)

    triples = test_dataset.triples
    if max_head_entities:
        triples = triples[:max_head_entities]

    with torch.no_grad():
        for h, r, _ in tqdm(triples, desc="RRF Predict"):
            try:
                h_id = torch.tensor([mapper.entity_to_id[h]], device=device)
                r_id = torch.tensor([mapper.relation_to_id[r]], device=device)
            except KeyError:
                # è‹¥å®ä½“æœªç™»å½•ï¼Œè¿”å› 10 ä¸ªè‡ªèº«ï¼ˆæˆ–éšæœºï¼‰
                preds = [h] * 10
                results.append('\t'.join([h, r] + preds))
                continue

            # ========== RRF èåˆ ==========
            rrf_scores = np.zeros(mapper.entity_count)
            for name, (model, weight) in models_with_weights.items():
                q = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()
                _, indices = index.search(q, 1000)
                candidate_ids = indices[0]
                for rank, idx in enumerate(candidate_ids):
                    rrf_scores[idx] += weight / (rrf_k + rank + 1)

            # ========== æ’åºå¹¶å– Top10 ==========
            ranked_indices = np.argsort(rrf_scores)[::-1]
            top10_ids = ranked_indices[:10]
            preds = [mapper.id_to_entity[i] for i in top10_ids]

            results.append('\t'.join([h, r] + preds))

    # ========== ä¿å­˜ç»“æœ ==========
    os.makedirs(os.path.dirname(OUTPUT_FILE_PATH), exist_ok=True)
    with open(OUTPUT_FILE_PATH, 'w', encoding='utf-8') as f:
        f.write('\n'.join(results) + '\n')

    print(f"âœ… RRF èåˆç»“æœå·²ä¿å­˜è‡³: {OUTPUT_FILE_PATH}")

    # å‹ç¼©ä¸º zip
    zip_path = OUTPUT_FILE_PATH.replace(".tsv", "") + f"__{scheme_type}.zip"
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
        zf.write(OUTPUT_FILE_PATH, arcname=os.path.basename(OUTPUT_FILE_PATH))
    print(f"âœ… å·²å‹ç¼©ä¸º: {zip_path}")


# ==================== ä¸»å‡½æ•° ====================
def main():
    device = torch.device('mps') if torch.backends.mps.is_available() else \
             torch.device('cpu')
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")

    train_data = KnowledgeGraphDataset(TRAIN_FILE_PATH, max_lines=MAX_LINES, is_train=True)
    dev_data = KnowledgeGraphDataset(DEV_FILE_PATH, is_test=False, is_train=False)
    test_data = KnowledgeGraphDataset(TEST_FILE_PATH, is_test=True, is_train=False)

    mapper = EntityRelationMapper()
    mapper.build_mappings(train_data, dev_data, test_data)
    print(f"å®ä½“æ•°: {mapper.entity_count}, å…³ç³»æ•°: {mapper.relation_count}")

    model_classes = {
        # 'TransE': TransE, ##ã€TODOã€‘å®é™…è·‘çš„æ—¶å€™è¦æ”¾å¼€çš„è¿™é‡Œã€‚
        'TransH': TransH,
        # 'TransD': TransD,
    }

    # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
    for name, Cls in model_classes.items():
        model = Cls(mapper.entity_count, mapper.relation_count, EMBEDDING_DIM)
        train_model(model, name, train_data, mapper, device)

    # åŠ è½½å¹¶è¯„ä¼°æ¯ä¸ªæ¨¡å‹
    loaded_models_with_weight = {}
    for name, Cls in model_classes.items():
        model = load_model(Cls, TRAINED_MODEL_PATHS[name], mapper, device)
        loaded_models_with_weight[name] = (model, 1.0)
        print(f"\nğŸ“ˆ æ­£åœ¨è¯„ä¼°æ¨¡å‹: {name}")
        ##ã€TODOã€‘å®é™…è·‘çš„æ—¶å€™è¦æ”¾å¼€çš„è¿™é‡Œã€‚
        evaluate_model(model, dev_data, mapper, device) 
    
    # ##ã€TODOã€‘å®é™…è·‘çš„æ—¶å€™è¦æ”¾å¼€çš„è¿™é‡Œã€‚
    # # ğŸ”¥ è¯„ä¼°èåˆæ¨¡å‹
    # evaluate_ensemble(loaded_models_with_weight, dev_data, mapper, device) 

    # ##ã€TODOã€‘å®é™…è·‘çš„æ—¶å€™è¦æ”¾å¼€çš„è¿™é‡Œã€‚
    # # æ‰§è¡Œèåˆé¢„æµ‹
    # predict_ensemble(loaded_models_with_weight, test_data, mapper, device, MAX_HEAD_ENTITIES)
    # print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼èåˆé¢„æµ‹åŠè¯„ä¼°å·²å®Œæˆã€‚")

if __name__ == "__main__":
    main()