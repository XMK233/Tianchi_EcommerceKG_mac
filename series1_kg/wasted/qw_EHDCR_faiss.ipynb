{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8726441-1a80-4377-a5ce-e1c0af4aeb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import faiss  # üî• ÂºïÂÖ• FAISS\n",
    "\n",
    "# ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠ê\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "scheme_type = \"ensemble_faiss\"\n",
    "\n",
    "# Êï∞ÊçÆË∑ØÂæÑÔºàËØ∑Ê†πÊçÆ‰Ω†ÁöÑÂÆûÈôÖË∑ØÂæÑ‰øÆÊîπÔºâ\n",
    "BASE_DIR = \"/Users/minkexiu/Downloads/GitHub/Tianchi_EcommerceKG_mac\"\n",
    "TRAIN_FILE_PATH = f\"{BASE_DIR}/originalData/OpenBG500/OpenBG500_train.tsv\"\n",
    "TEST_FILE_PATH = f\"{BASE_DIR}/originalData/OpenBG500/OpenBG500_test.tsv\"\n",
    "DEV_FILE_PATH = f\"{BASE_DIR}/originalData/OpenBG500/OpenBG500_dev.tsv\"\n",
    "OUTPUT_FILE_PATH = f\"{BASE_DIR}/preprocessedData/OpenBG500_test__{scheme_type}.tsv\"\n",
    "\n",
    "# Ê®°Âûã‰øùÂ≠òË∑ØÂæÑ\n",
    "MODEL_DIR = f\"{BASE_DIR}/trained_model\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Ê®°ÂûãË∑ØÂæÑ\n",
    "TRAINED_MODEL_PATHS = {\n",
    "    'TransE': f\"{MODEL_DIR}/trained_model__transE.pth\",\n",
    "    'TransH': f\"{MODEL_DIR}/trained_model__transH.pth\",\n",
    "    'TransD': f\"{MODEL_DIR}/trained_model__transD.pth\",\n",
    "    'ConvE': f\"{MODEL_DIR}/trained_model__conve.pth\",\n",
    "    'RotatE': f\"{MODEL_DIR}/trained_model__rotate.pth\"\n",
    "}\n",
    "\n",
    "# Ë∂ÖÂèÇÊï∞\n",
    "EMBEDDING_DIM = 100\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 256\n",
    "NEGATIVE_SAMPLES = 10\n",
    "MAX_LINES = None\n",
    "MAX_HEAD_ENTITIES = None\n",
    "LR_DECAY_STEP = 5\n",
    "LR_DECAY_FACTOR = 0.1\n",
    "\n",
    "\n",
    "# ==================== Êï∞ÊçÆÈõÜ ====================\n",
    "class KnowledgeGraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path, is_test=False, max_lines=None, is_train=False):\n",
    "        self.triples = []\n",
    "        self.is_train = is_train\n",
    "        self._load_data(file_path, is_test, max_lines)\n",
    "\n",
    "    def _load_data(self, file_path, is_test, max_lines):\n",
    "        print(f\"Âä†ËΩΩÊï∞ÊçÆ: {file_path}\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            if max_lines:\n",
    "                lines = lines[:max_lines]\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 3:\n",
    "                    h, r, t = parts\n",
    "                    self.triples.append((h, r, t))\n",
    "                elif is_test and len(parts) == 2:\n",
    "                    h, r = parts\n",
    "                    self.triples.append((h, r, \"<UNK>\"))\n",
    "        print(f\"ÂÖ±Âä†ËΩΩ {len(self.triples)} ‰∏™‰∏âÂÖÉÁªÑ\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.triples[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    h_list, r_list, t_list = zip(*batch)\n",
    "    return list(h_list), list(r_list), list(t_list)\n",
    "\n",
    "\n",
    "# ==================== Êò†Â∞ÑÂô® ====================\n",
    "class EntityRelationMapper:\n",
    "    def __init__(self):\n",
    "        self.entity_to_id = {}\n",
    "        self.id_to_entity = {}\n",
    "        self.relation_to_id = {}\n",
    "        self.id_to_relation = {}\n",
    "        self.entity_count = 0\n",
    "        self.relation_count = 0\n",
    "        self.all_train_triples = []\n",
    "\n",
    "    def build_mappings(self, *datasets):\n",
    "        entities = set()\n",
    "        relations = set()\n",
    "        for dataset in datasets:\n",
    "            for h, r, t in dataset.triples:\n",
    "                entities.add(h)\n",
    "                entities.add(t)\n",
    "                relations.add(r)\n",
    "                if dataset.is_train:\n",
    "                    self.all_train_triples.append((h, r, t))\n",
    "\n",
    "        for e in sorted(entities):\n",
    "            self.entity_to_id[e] = self.entity_count\n",
    "            self.id_to_entity[self.entity_count] = e\n",
    "            self.entity_count += 1\n",
    "        for r in sorted(relations):\n",
    "            self.relation_to_id[r] = self.relation_count\n",
    "            self.id_to_relation[self.relation_count] = r\n",
    "            self.relation_count += 1\n",
    "\n",
    "\n",
    "# ==================== TransE ====================\n",
    "class TransE(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, dim):\n",
    "        super().__init__()\n",
    "        self.E = nn.Embedding(num_entities, dim)\n",
    "        self.R = nn.Embedding(num_relations, dim)\n",
    "        nn.init.xavier_uniform_(self.E.weight)\n",
    "        nn.init.xavier_uniform_(self.R.weight)\n",
    "\n",
    "    def forward(self, h, r, t):\n",
    "        return torch.norm(self.E(h) + self.R(r) - self.E(t), p=1, dim=1)\n",
    "\n",
    "    def get_query_embedding(self, h, r):\n",
    "        return self.E(h) + self.R(r)\n",
    "\n",
    "\n",
    "# ==================== TransH ====================\n",
    "class TransH(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, dim):\n",
    "        super().__init__()\n",
    "        self.E = nn.Embedding(num_entities, dim)\n",
    "        self.R = nn.Embedding(num_relations, dim)\n",
    "        self.W = nn.Embedding(num_relations, dim)\n",
    "        nn.init.xavier_uniform_(self.E.weight)\n",
    "        nn.init.xavier_uniform_(self.R.weight)\n",
    "        nn.init.xavier_uniform_(self.W.weight)\n",
    "\n",
    "    def project(self, emb, norm):\n",
    "        norm = torch.nn.functional.normalize(norm, p=2, dim=1)\n",
    "        return emb - torch.sum(emb * norm, dim=1, keepdim=True) * norm\n",
    "\n",
    "    def forward(self, h, r, t):\n",
    "        h_emb = self.project(self.E(h), self.W(r))\n",
    "        t_emb = self.project(self.E(t), self.W(r))\n",
    "        return torch.norm(h_emb + self.R(r) - t_emb, p=1, dim=1)\n",
    "\n",
    "    def get_query_embedding(self, h, r):\n",
    "        h_emb = self.project(self.E(h), self.W(r))\n",
    "        r_vec = self.R(r)\n",
    "        return h_emb + r_vec\n",
    "\n",
    "\n",
    "# ==================== TransD ====================\n",
    "class TransD(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.E = nn.Embedding(num_entities, dim)\n",
    "        self.R = nn.Embedding(num_relations, dim)\n",
    "        self.E_proj = nn.Embedding(num_entities, dim)\n",
    "        self.R_proj = nn.Embedding(num_relations, dim)\n",
    "        nn.init.xavier_uniform_(self.E.weight)\n",
    "        nn.init.xavier_uniform_(self.R.weight)\n",
    "        nn.init.xavier_uniform_(self.E_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.R_proj.weight)\n",
    "\n",
    "    def project(self, e, r_proj):\n",
    "        return e + torch.sum(e * r_proj, dim=1, keepdim=True)\n",
    "\n",
    "    def forward(self, h, r, t):\n",
    "        h_emb = self.project(self.E(h), self.R_proj(r))\n",
    "        t_emb = self.project(self.E(t), self.R_proj(r))\n",
    "        r_vec = self.R(r)\n",
    "        return torch.norm(h_emb + r_vec - t_emb, p=1, dim=1)\n",
    "\n",
    "    def get_query_embedding(self, h, r):\n",
    "        h_emb = self.project(self.E(h), self.R_proj(r))\n",
    "        r_vec = self.R(r)\n",
    "        return h_emb + r_vec\n",
    "\n",
    "\n",
    "# ==================== ConvE ====================\n",
    "class ConvE(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, dim, embedding_dim=100, feature_map_dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hid_drop = feature_map_dropout\n",
    "\n",
    "        # ÂµåÂÖ•Â±Ç\n",
    "        self.E = nn.Embedding(num_entities, embedding_dim)\n",
    "        self.R = nn.Embedding(num_relations, embedding_dim)\n",
    "\n",
    "        # Âç∑ÁßØÂ±Ç\n",
    "        self.in_channels = 1\n",
    "        self.out_channels = 32\n",
    "        self.kernel_size = 3\n",
    "        self.padding = 0\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=self.in_channels,\n",
    "                               out_channels=self.out_channels,\n",
    "                               kernel_size=(self.kernel_size, self.kernel_size),\n",
    "                               padding=self.padding)\n",
    "\n",
    "        self.fc_dim = (embedding_dim - self.kernel_size + 2 * self.padding + 1) ** 2 * self.out_channels\n",
    "        self.fc = nn.Linear(self.fc_dim, embedding_dim)\n",
    "\n",
    "        self.h_bn = nn.BatchNorm2d(1)\n",
    "        self.r_bn = nn.BatchNorm1d(embedding_dim)\n",
    "        self.dropout = nn.Dropout(self.hid_drop)\n",
    "        self.hidden_drop = nn.Dropout(self.hid_drop)\n",
    "\n",
    "        nn.init.xavier_normal_(self.E.weight)\n",
    "        nn.init.xavier_normal_(self.R.weight)\n",
    "\n",
    "    def forward(self, h, r, t):\n",
    "        h_emb = self.E(h).view(-1, 1, self.embedding_dim, 1)\n",
    "        r_emb = self.R(r).view(-1, 1, 1, self.embedding_dim)\n",
    "\n",
    "        h_emb = self.h_bn(h_emb)\n",
    "        r_emb = self.r_bn(r_emb)\n",
    "\n",
    "        stacked = h_emb * r_emb  # [B, 1, dim, dim]\n",
    "\n",
    "        x = self.conv1(stacked)\n",
    "        x = torch.relu(x)\n",
    "        x = self.hidden_drop(x)\n",
    "        x = x.view(-1, self.fc_dim)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.nn.functional.normalize(x, p=2, dim=1)\n",
    "\n",
    "        t_emb = self.E(t)\n",
    "        return torch.sum(-torch.norm(x - t_emb, p=1, dim=1), dim=0)\n",
    "\n",
    "    def get_query_embedding(self, h, r):\n",
    "        h_emb = self.E(h).view(-1, 1, self.embedding_dim, 1)\n",
    "        r_emb = self.R(r).view(-1, 1, 1, self.embedding_dim)\n",
    "\n",
    "        h_emb = self.h_bn(h_emb)\n",
    "        r_emb = self.r_bn(r_emb)\n",
    "\n",
    "        stacked = h_emb * r_emb\n",
    "        x = self.conv1(stacked)\n",
    "        x = torch.relu(x)\n",
    "        x = self.hidden_drop(x)\n",
    "        x = x.view(-1, self.fc_dim)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        return x  # [B, dim]\n",
    "\n",
    "\n",
    "# ==================== RotatE ====================\n",
    "class RotatE(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, dim, gamma=12.0):\n",
    "        super().__init__()\n",
    "        self.E = nn.Embedding(num_entities, dim)\n",
    "        self.R = nn.Embedding(num_relations, dim)\n",
    "        self.gamma = gamma\n",
    "        self.embedding_dim = dim\n",
    "\n",
    "        nn.init.uniform_(self.E.weight, -6 / dim ** 0.5, 6 / dim ** 0.5)\n",
    "        nn.init.uniform_(self.R.weight, -6 / dim ** 0.5, 6 / dim ** 0.5)\n",
    "        self.E.weight.data = self.E.weight.data / (self.E.weight.data.norm(p=2, dim=1, keepdim=True))\n",
    "        self.R.weight.data = self.R.weight.data / (self.R.weight.data.norm(p=2, dim=1, keepdim=True))\n",
    "\n",
    "    def forward(self, h, r, t):\n",
    "        re_head, im_head = torch.chunk(self.E(h), 2, dim=1)\n",
    "        re_tail, im_tail = torch.chunk(self.E(t), 2, dim=1)\n",
    "\n",
    "        # Relation as rotation in complex plane\n",
    "        r_emb = self.R(r)\n",
    "        r_phase = r_emb / (self.embedding_dim / 2)\n",
    "        r_phase = r_phase * (3.14159265358979323846)\n",
    "        re_relation = torch.cos(r_phase)\n",
    "        im_relation = torch.sin(r_phase)\n",
    "\n",
    "        # (re_head + i*im_head) * (re_relation + i*im_relation) = ?\n",
    "        re_score = re_head * re_relation - im_head * im_relation\n",
    "        im_score = re_head * im_relation + im_head * re_relation\n",
    "        re_score = re_score - re_tail\n",
    "        im_score = im_score - im_tail\n",
    "\n",
    "        score = torch.stack([re_score, im_score], dim=0)\n",
    "        score = score.norm(dim=0)\n",
    "        return torch.sum(score, dim=1)\n",
    "\n",
    "    def get_query_embedding(self, h, r):\n",
    "        re_head, im_head = torch.chunk(self.E(h), 2, dim=1)\n",
    "        r_emb = self.R(r)\n",
    "        r_phase = r_emb / (self.embedding_dim / 2)\n",
    "        r_phase = r_phase * 3.14159265358979323846\n",
    "        re_relation = torch.cos(r_phase)\n",
    "        im_relation = torch.sin(r_phase)\n",
    "\n",
    "        re_score = re_head * re_relation - im_head * im_relation\n",
    "        im_score = re_head * im_relation + im_head * re_relation\n",
    "        return torch.cat([re_score, im_score], dim=1)  # [B, dim]\n",
    "\n",
    "\n",
    "# ==================== ËÆ≠ÁªÉ & Âä†ËΩΩ & ËØÑ‰º∞ÔºàÂêåÂâçÔºâ====================\n",
    "def train_model(model, model_name, train_dataset, mapper, device):\n",
    "    if os.path.exists(TRAINED_MODEL_PATHS[model_name]):\n",
    "        print(f\"[{model_name}] Â∑≤Â≠òÂú®ËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÔºåË∑≥ËøáËÆ≠ÁªÉ\")\n",
    "        return\n",
    "    print(f\"[{model_name}] ÂºÄÂßãËÆ≠ÁªÉ...\")\n",
    "    loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_DECAY_STEP, gamma=LR_DECAY_FACTOR)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        progress = tqdm(loader, desc=f\"{model_name} Epoch {epoch+1}\")\n",
    "        for h_list, r_list, t_list in progress:\n",
    "            h = torch.tensor([mapper.entity_to_id[h] for h in h_list], device=device)\n",
    "            r = torch.tensor([mapper.relation_to_id[r] for r in r_list], device=device)\n",
    "            t = torch.tensor([mapper.entity_to_id[t] for t in t_list], device=device)\n",
    "            neg_t = torch.randint(0, mapper.entity_count, (len(h), NEGATIVE_SAMPLES), device=device)\n",
    "            pos_score = model(h, r, t)\n",
    "            neg_score = model(h.unsqueeze(1).expand(-1, NEGATIVE_SAMPLES).contiguous().view(-1),\n",
    "                              r.unsqueeze(1).expand(-1, NEGATIVE_SAMPLES).contiguous().view(-1),\n",
    "                              neg_t.view(-1)).view(-1, NEGATIVE_SAMPLES)\n",
    "            loss = torch.mean(torch.relu(pos_score.unsqueeze(1) - neg_score + 1.0))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            progress.set_postfix(loss=loss.item())\n",
    "        scheduler.step()\n",
    "        print(f\"[{model_name}] Epoch {epoch+1} Loss: {epoch_loss / len(loader):.4f}\")\n",
    "\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'entity_count': mapper.entity_count,\n",
    "        'relation_count': mapper.relation_count,\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'entity_to_id': mapper.entity_to_id,\n",
    "        'relation_to_id': mapper.relation_to_id,\n",
    "    }, TRAINED_MODEL_PATHS[model_name])\n",
    "    print(f\"[{model_name}] Ê®°ÂûãÂ∑≤‰øùÂ≠ò\")\n",
    "\n",
    "\n",
    "def load_model(model_class, model_path, mapper, device):\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model = model_class(checkpoint['entity_count'], checkpoint['relation_count'], EMBEDDING_DIM)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataset, mapper, device, k_list=(1, 3, 10)):\n",
    "    print(\"üîç ÂºÄÂßãÂú®ÂºÄÂèëÈõÜ‰∏äËØÑ‰º∞Ê®°ÂûãÊÄßËÉΩ...\")\n",
    "    model.eval()\n",
    "    hits_at = {k: 0.0 for k in k_list}\n",
    "    mrr = 0.0\n",
    "    count = 0\n",
    "\n",
    "    # ÊûÑÂª∫ FAISS Á¥¢ÂºïÔºàÁî®‰∫éÂä†ÈÄüÊêúÁ¥¢Ôºâ\n",
    "    entity_emb = model.E.weight.data.cpu().numpy()\n",
    "    index = faiss.IndexFlatL2(entity_emb.shape[1])  # L2 Ë∑ùÁ¶ª\n",
    "    index.add(entity_emb)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for h, r, t in tqdm(dataset.triples, desc=\"Evaluating\"):\n",
    "            try:\n",
    "                h_id = torch.tensor([mapper.entity_to_id[h]], device=device)\n",
    "                r_id = torch.tensor([mapper.relation_to_id[r]], device=device)\n",
    "                t_id = torch.tensor([mapper.entity_to_id[t]], device=device)\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            # ========== Â∞æÂÆû‰ΩìÈ¢ÑÊµã ==========\n",
    "            query = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "\n",
    "            _, indices = index.search(query, 1000)  # ÊêúÁ¥¢Ââç 1000 Âêç\n",
    "            pred_ids = indices[0]\n",
    "            rank = np.where(pred_ids == mapper.entity_to_id[t])[0]\n",
    "            rank = rank[0] + 1 if len(rank) > 0 else 10000\n",
    "\n",
    "            for k in k_list:\n",
    "                if rank <= k:\n",
    "                    hits_at[k] += 1\n",
    "            mrr += 1.0 / rank\n",
    "            count += 1\n",
    "\n",
    "            # ========== Â§¥ÂÆû‰ΩìÈ¢ÑÊµã ==========\n",
    "            if hasattr(model, 'get_query_embedding_for_head'):\n",
    "                query_h = model.get_query_embedding_for_head(r_id, t_id).detach().cpu().numpy()\n",
    "            else:\n",
    "                query_h = (model.E(t_id) - model.R(r_id)).detach().cpu().numpy()\n",
    "\n",
    "            _, indices_h = index.search(query_h, 1000)\n",
    "            pred_ids_h = indices_h[0]\n",
    "            rank_h = np.where(pred_ids_h == mapper.entity_to_id[h])[0]\n",
    "            rank_h = rank_h[0] + 1 if len(rank_h) > 0 else 10000\n",
    "\n",
    "            for k in k_list:\n",
    "                if rank_h <= k:\n",
    "                    hits_at[k] += 1\n",
    "            mrr += 1.0 / rank_h\n",
    "            count += 1\n",
    "\n",
    "    for k in hits_at:\n",
    "        hits_at[k] /= count\n",
    "    mrr /= count\n",
    "\n",
    "    print(\"‚úÖ ËØÑ‰º∞ÂÆåÊàêÔºÅ\")\n",
    "    print(f\"üìä HITS@1:  {hits_at[1]:.4f}\")\n",
    "    print(f\"üìä HITS@3:  {hits_at[3]:.4f}\")\n",
    "    print(f\"üìä HITS@10: {hits_at[10]:.4f}\")\n",
    "    print(f\"üìä MRR:     {mrr:.4f}\")\n",
    "\n",
    "    return hits_at, mrr\n",
    "\n",
    "\n",
    "# ==================== ËûçÂêàÈ¢ÑÊµã + FAISS Âä†ÈÄü ====================\n",
    "def predict_ensemble(models_with_weights, test_dataset, mapper, device, max_head_entities=None):\n",
    "    print(\"üîç ÂºÄÂßãËûçÂêàÈ¢ÑÊµã (Âä†ÊùÉËûçÂêà + FAISS Âä†ÈÄü) ...\")\n",
    "    results = []\n",
    "\n",
    "    # ÊâÄÊúâÂÆû‰ΩìÂµåÂÖ•ÔºàÁªü‰∏ÄÁª¥Â∫¶Ôºâ\n",
    "    all_entity_emb = models_with_weights['TransE'][0].E.weight.data.cpu().numpy()  # ÂÅáËÆæÁª¥Â∫¶‰∏ÄËá¥\n",
    "    index = faiss.IndexFlatL2(all_entity_emb.shape[1])\n",
    "    index.add(all_entity_emb)\n",
    "\n",
    "    triples = test_dataset.triples\n",
    "    if max_head_entities:\n",
    "        triples = triples[:max_head_entities]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for h, r, _ in tqdm(triples, desc=\"Ensemble Predict\"):\n",
    "            try:\n",
    "                h_id = torch.tensor([mapper.entity_to_id[h]], device=device)\n",
    "                r_id = torch.tensor([mapper.relation_to_id[r]], device=device)\n",
    "            except KeyError:\n",
    "                preds = [h] * 10\n",
    "                results.append('\\t'.join([h, r] + preds))\n",
    "                continue\n",
    "\n",
    "            # ËûçÂêàÊü•ËØ¢ÂêëÈáè\n",
    "            fused_query = np.zeros((1, EMBEDDING_DIM))\n",
    "            for name, (model, weight) in models_with_weights.items():\n",
    "                q = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "                fused_query += weight * q\n",
    "\n",
    "            # FAISS ÊêúÁ¥¢ Top-10\n",
    "            _, indices = index.search(fused_query, 10)\n",
    "            pred_ids = indices[0]\n",
    "            preds = [mapper.id_to_entity[i] for i in pred_ids]\n",
    "            results.append('\\t'.join([h, r] + preds))\n",
    "\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE_PATH), exist_ok=True)\n",
    "    with open(OUTPUT_FILE_PATH, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(results) + '\\n')\n",
    "    print(f\"‚úÖ ËûçÂêàÁªìÊûúÂ∑≤‰øùÂ≠òËá≥: {OUTPUT_FILE_PATH}\")\n",
    "\n",
    "    zip_path = OUTPUT_FILE_PATH.replace(\".tsv\", \".zip\")\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        zf.write(OUTPUT_FILE_PATH, arcname=os.path.basename(OUTPUT_FILE_PATH))\n",
    "    print(f\"‚úÖ Â∑≤ÂéãÁº©‰∏∫: {zip_path}\")\n",
    "\n",
    "\n",
    "# ==================== ‰∏ªÂáΩÊï∞ ====================\n",
    "def main():\n",
    "    device = torch.device('mps') if torch.backends.mps.is_available() else \\\n",
    "             torch.device('cpu')\n",
    "    print(f\"üöÄ ‰ΩøÁî®ËÆæÂ§á: {device}\")\n",
    "\n",
    "    train_data = KnowledgeGraphDataset(TRAIN_FILE_PATH, max_lines=MAX_LINES, is_train=True)\n",
    "    dev_data = KnowledgeGraphDataset(DEV_FILE_PATH, is_test=False, is_train=False)\n",
    "    test_data = KnowledgeGraphDataset(TEST_FILE_PATH, is_test=True, is_train=False)\n",
    "\n",
    "    mapper = EntityRelationMapper()\n",
    "    mapper.build_mappings(train_data, dev_data, test_data)\n",
    "    print(f\"ÂÆû‰ΩìÊï∞: {mapper.entity_count}, ÂÖ≥Á≥ªÊï∞: {mapper.relation_count}\")\n",
    "\n",
    "    model_classes = {\n",
    "        'TransE': TransE,\n",
    "        'TransH': TransH,\n",
    "        'TransD': TransD,\n",
    "        'ConvE': ConvE,\n",
    "        'RotatE': RotatE,\n",
    "    }\n",
    "\n",
    "    # ËÆ≠ÁªÉÊâÄÊúâÊ®°Âûã\n",
    "    for name, Cls in model_classes.items():\n",
    "        model = Cls(mapper.entity_count, mapper.relation_count, EMBEDDING_DIM)\n",
    "        train_model(model, name, train_data, mapper, device)\n",
    "\n",
    "    # Âä†ËΩΩ & ËØÑ‰º∞ & ËûçÂêà\n",
    "    loaded_models_with_weight = {}\n",
    "    for name, Cls in model_classes.items():\n",
    "        model = load_model(Cls, TRAINED_MODEL_PATHS[name], mapper, device)\n",
    "        loaded_models_with_weight[name] = (model, 1.0)\n",
    "        print(f\"\\nüìà Ê≠£Âú®ËØÑ‰º∞Ê®°Âûã: {name}\")\n",
    "        evaluate_model(model, dev_data, mapper, device)\n",
    "\n",
    "    # ËûçÂêàÈ¢ÑÊµã\n",
    "    predict_ensemble(loaded_models_with_weight, test_data, mapper, device, MAX_HEAD_ENTITIES)\n",
    "    print(\"üéâ ÊâÄÊúâ‰ªªÂä°ÂÆåÊàêÔºÅËûçÂêàÈ¢ÑÊµãÂ∑≤ÁîüÊàêÔºåÂºÄÂèëÈõÜËØÑ‰º∞Â∑≤ÂÆåÊàê„ÄÇ\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
