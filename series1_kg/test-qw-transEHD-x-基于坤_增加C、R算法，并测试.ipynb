{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3771edd-8b12-4ee9-9d8a-be3c32bbe6a3",
   "metadata": {},
   "source": [
    "# è·‘æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74374859-f1a6-457d-ad30-f696d8b7e0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ä½¿ç”¨è®¾å¤‡: mps\n",
      "åŠ è½½æ•°æ®: /Users/minkexiu/Downloads/GitHub/Tianchi_EcommerceKG_mac/originalData/OpenBG500/OpenBG500_train.tsv\n",
      "å…±åŠ è½½ 1242550 ä¸ªä¸‰å…ƒç»„\n",
      "åŠ è½½æ•°æ®: /Users/minkexiu/Downloads/GitHub/Tianchi_EcommerceKG_mac/originalData/OpenBG500/OpenBG500_dev.tsv\n",
      "å…±åŠ è½½ 5000 ä¸ªä¸‰å…ƒç»„\n",
      "åŠ è½½æ•°æ®: /Users/minkexiu/Downloads/GitHub/Tianchi_EcommerceKG_mac/originalData/OpenBG500/OpenBG500_test.tsv\n",
      "å…±åŠ è½½ 5000 ä¸ªä¸‰å…ƒç»„\n",
      "å®ä½“æ•°: 249747, å…³ç³»æ•°: 500\n",
      "[RotatE] å·²å­˜åœ¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè·³è¿‡è®­ç»ƒ\n",
      "\n",
      "ğŸ“ˆ æ­£åœ¨è¯„ä¼°æ¨¡å‹: RotatE\n",
      "ğŸ” å¼€å§‹åœ¨å¼€å‘é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ˆä»…å°¾å®ä½“é¢„æµ‹ï¼‰...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RotatE' object has no attribute 'E'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 621\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;66;03m# ##ã€TODOã€‘å®é™…è·‘çš„æ—¶å€™è¦æ”¾å¼€çš„è¿™é‡Œã€‚\u001b[39;00m\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;66;03m# # ğŸ”¥ è¯„ä¼°èåˆæ¨¡å‹\u001b[39;00m\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;66;03m# evaluate_ensemble(loaded_models_with_weight, dev_data, mapper, device) \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;66;03m# predict_ensemble(loaded_models_with_weight, test_data, mapper, device, MAX_HEAD_ENTITIES)\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;66;03m# print(\"ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼èåˆé¢„æµ‹åŠè¯„ä¼°å·²å®Œæˆã€‚\")\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 621\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 609\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸ“ˆ æ­£åœ¨è¯„ä¼°æ¨¡å‹: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    608\u001b[0m \u001b[38;5;66;03m##ã€TODOã€‘å®é™…è·‘çš„æ—¶å€™è¦æ”¾å¼€çš„è¿™é‡Œã€‚\u001b[39;00m\n\u001b[0;32m--> 609\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 379\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataset, mapper, device, k_list)\u001b[0m\n\u001b[1;32m    376\u001b[0m mrr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    377\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 379\u001b[0m entity_emb \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mE\u001b[49m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    380\u001b[0m index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatL2(entity_emb\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    381\u001b[0m index\u001b[38;5;241m.\u001b[39madd(entity_emb)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml12/lib/python3.12/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RotatE' object has no attribute 'E'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import faiss  # åŠ é€Ÿ Top-K æœç´¢\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "scheme_type = \"test_c_r\"\n",
    "\n",
    "# æ•°æ®è·¯å¾„ï¼ˆè¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹ï¼‰\n",
    "BASE_DIR = \"/Users/minkexiu/Downloads/GitHub/Tianchi_EcommerceKG_mac\"\n",
    "TRAIN_FILE_PATH = f\"{BASE_DIR}/originalData/OpenBG500/OpenBG500_train.tsv\"\n",
    "TEST_FILE_PATH = f\"{BASE_DIR}/originalData/OpenBG500/OpenBG500_test.tsv\"\n",
    "DEV_FILE_PATH = f\"{BASE_DIR}/originalData/OpenBG500/OpenBG500_dev.tsv\"\n",
    "OUTPUT_FILE_PATH = f\"{BASE_DIR}/preprocessedData/OpenBG500_test.tsv\" \n",
    "\n",
    "# æ¨¡å‹ä¿å­˜è·¯å¾„\n",
    "MODEL_DIR = f\"{BASE_DIR}/trained_models/{scheme_type}\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# æ¨¡å‹è·¯å¾„ \n",
    "TRAINED_MODEL_PATHS = {\n",
    "    'TransE': f\"{MODEL_DIR}/transE.pth\",\n",
    "    'TransH': f\"{MODEL_DIR}/transH.pth\",\n",
    "    'TransD': f\"{MODEL_DIR}/transD.pth\",\n",
    "\n",
    "    'RotatE': f\"{MODEL_DIR}/RotatE.pth\",\n",
    "    'ConvE': f\"{MODEL_DIR}/ConvE.pth\",\n",
    "}\n",
    "\n",
    "# è¶…å‚æ•°\n",
    "EMBEDDING_DIM = 100\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 1 ##ã€TODOã€‘è¿™é‡Œå¯ä»¥ä¿®æ”¹å¤šä¸€ç‚¹ã€‚\n",
    "BATCH_SIZE = 256\n",
    "NEGATIVE_SAMPLES = 10\n",
    "MAX_LINES = None\n",
    "MAX_HEAD_ENTITIES = None\n",
    "LR_DECAY_STEP = 5\n",
    "LR_DECAY_FACTOR = 0.1\n",
    "\n",
    "\n",
    "# ==================== æ•°æ®é›† ====================\n",
    "class KnowledgeGraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path, is_test=False, max_lines=None, is_train=False):\n",
    "        self.triples = []\n",
    "        self.is_train = is_train\n",
    "        self._load_data(file_path, is_test, max_lines)\n",
    "\n",
    "    def _load_data(self, file_path, is_test, max_lines):\n",
    "        print(f\"åŠ è½½æ•°æ®: {file_path}\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            if max_lines:\n",
    "                lines = lines[:max_lines]\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 3:\n",
    "                    h, r, t = parts\n",
    "                    self.triples.append((h, r, t))\n",
    "                elif is_test and len(parts) == 2:\n",
    "                    h, r = parts\n",
    "                    self.triples.append((h, r, \"<UNK>\"))\n",
    "        print(f\"å…±åŠ è½½ {len(self.triples)} ä¸ªä¸‰å…ƒç»„\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.triples[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    h_list, r_list, t_list = zip(*batch)\n",
    "    return list(h_list), list(r_list), list(t_list)\n",
    "\n",
    "\n",
    "# ==================== æ˜ å°„å™¨ ====================\n",
    "class EntityRelationMapper:\n",
    "    def __init__(self):\n",
    "        self.entity_to_id = {}\n",
    "        self.id_to_entity = {}\n",
    "        self.relation_to_id = {}\n",
    "        self.id_to_relation = {}\n",
    "        self.entity_count = 0\n",
    "        self.relation_count = 0\n",
    "        self.all_train_triples = []\n",
    "\n",
    "    def build_mappings(self, *datasets):\n",
    "        entities = set()\n",
    "        relations = set()\n",
    "        for dataset in datasets:\n",
    "            for h, r, t in dataset.triples:\n",
    "                entities.add(h)\n",
    "                entities.add(t)\n",
    "                relations.add(r)\n",
    "                if dataset.is_train:\n",
    "                    self.all_train_triples.append((h, r, t))\n",
    "\n",
    "        for e in sorted(entities):\n",
    "            self.entity_to_id[e] = self.entity_count\n",
    "            self.id_to_entity[self.entity_count] = e\n",
    "            self.entity_count += 1\n",
    "        for r in sorted(relations):\n",
    "            self.relation_to_id[r] = self.relation_count\n",
    "            self.id_to_relation[self.relation_count] = r\n",
    "            self.relation_count += 1\n",
    "\n",
    "\n",
    "# ==================== TransE ====================\n",
    "class TransE(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, dim):\n",
    "        super().__init__()\n",
    "        self.E = nn.Embedding(num_entities, dim)\n",
    "        self.R = nn.Embedding(num_relations, dim)\n",
    "        nn.init.xavier_uniform_(self.E.weight)\n",
    "        nn.init.xavier_uniform_(self.R.weight)\n",
    "\n",
    "    def forward(self, h, r, t):\n",
    "        return torch.norm(self.E(h) + self.R(r) - self.E(t), p=1, dim=1)\n",
    "\n",
    "    def get_query_embedding(self, h, r):\n",
    "        return self.E(h) + self.R(r)\n",
    "\n",
    "\n",
    "# ==================== TransH ï¼ˆå·²ä¿®å¤ï¼‰====================\n",
    "class TransH(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, dim):\n",
    "        super().__init__()\n",
    "        self.E = nn.Embedding(num_entities, dim)\n",
    "        self.R = nn.Embedding(num_relations, dim)      # å…³ç³»å‘é‡ d_r\n",
    "        self.W = nn.Embedding(num_relations, dim)    # æ³•å‘é‡ W (ç”¨äºè¶…å¹³é¢)\n",
    "        nn.init.xavier_uniform_(self.E.weight)\n",
    "        nn.init.xavier_uniform_(self.R.weight)\n",
    "        nn.init.xavier_uniform_(self.W.weight)\n",
    "\n",
    "    def project(self, emb, w):  # å°†å®ä½“æŠ•å½±åˆ°å…³ç³»è¶…å¹³é¢ä¸Š\n",
    "        # w: [B, dim], emb: [B, dim]\n",
    "        norm_w = torch.nn.functional.normalize(w, p=2, dim=1)  # å•ä½åŒ–æ³•å‘é‡\n",
    "        scale = torch.sum(emb * norm_w, dim=1, keepdim=True)  # <e, w>\n",
    "        return emb - scale * norm_w  # e - <e, w> * w\n",
    "\n",
    "    def forward(self, h, r, t):\n",
    "        h_emb = self.E(h)  # [B, dim]\n",
    "        t_emb = self.E(t)\n",
    "        r_vec = self.R(r)\n",
    "        W = self.W(r)\n",
    "\n",
    "        h_proj = self.project(h_emb, W)\n",
    "        t_proj = self.project(t_emb, W)\n",
    "\n",
    "        return torch.norm(h_proj + r_vec - t_proj, p=1, dim=1)\n",
    "\n",
    "    def get_query_embedding(self, h, r):\n",
    "        h_emb = self.E(h)\n",
    "        r_vec = self.R(r)\n",
    "        W = self.W(r)\n",
    "        h_proj = self.project(h_emb, W)\n",
    "        return h_proj + r_vec  # æŸ¥è¯¢å‘é‡\n",
    "\n",
    "\n",
    "# ==================== TransD ====================\n",
    "class TransD(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.E = nn.Embedding(num_entities, dim)\n",
    "        self.R = nn.Embedding(num_relations, dim)\n",
    "        self.E_proj = nn.Embedding(num_entities, dim)\n",
    "        self.R_proj = nn.Embedding(num_relations, dim)\n",
    "        nn.init.xavier_uniform_(self.E.weight)\n",
    "        nn.init.xavier_uniform_(self.R.weight)\n",
    "        nn.init.xavier_uniform_(self.E_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.R_proj.weight)\n",
    "\n",
    "    def project(self, e, r_proj):\n",
    "        return e + torch.sum(e * r_proj, dim=1, keepdim=True)\n",
    "\n",
    "    def forward(self, h, r, t):\n",
    "        h_emb = self.project(self.E(h), self.R_proj(r))\n",
    "        t_emb = self.project(self.E(t), self.R_proj(r))\n",
    "        r_vec = self.R(r)\n",
    "        return torch.norm(h_emb + r_vec - t_emb, p=1, dim=1)\n",
    "\n",
    "    def get_query_embedding(self, h, r):\n",
    "        h_emb = self.project(self.E(h), self.R_proj(r))\n",
    "        r_vec = self.R(r)\n",
    "        return h_emb + r_vec\n",
    "\n",
    "# ==================== RotatE ====================\n",
    "class RotatE(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.E_re = nn.Embedding(num_entities, dim)\n",
    "        self.E_im = nn.Embedding(num_entities, dim)\n",
    "        self.R_re = nn.Embedding(num_relations, dim)\n",
    "        self.R_im = nn.Embedding(num_relations, dim)\n",
    "        \n",
    "        # åˆå§‹åŒ–ï¼šå®ä½“åœ¨å•ä½åœ†ä¸Šï¼Œå…³ç³»ä¸ºæ—‹è½¬è§’åº¦\n",
    "        nn.init.xavier_uniform_(self.E_re.weight)\n",
    "        nn.init.xavier_uniform_(self.E_im.weight)\n",
    "        nn.init.xavier_uniform_(self.R_re.weight)\n",
    "        nn.init.xavier_uniform_(self.R_im.weight)\n",
    "        \n",
    "        # å½’ä¸€åŒ–æ¨¡é•¿ä¸º 1ï¼ˆå•ä½å¤æ•°ï¼‰\n",
    "        self.E_re.weight.data = self.E_re.weight.data / (self.E_re.weight.data.norm(p=2, dim=1, keepdim=True) + 1e-9)\n",
    "        self.E_im.weight.data = self.E_im.weight.data / (self.E_im.weight.data.norm(p=2, dim=1, keepdim=True) + 1e-9)\n",
    "\n",
    "    def forward(self, h, r, t):\n",
    "        # å¤æ•°ä¹˜æ³•: (h_re + i h_im) * (r_re + i r_im) â‰ˆ t_re + i t_im\n",
    "        h_re = self.E_re(h)\n",
    "        h_im = self.E_im(h)\n",
    "        r_re = self.R_re(r)\n",
    "        r_im = self.R_im(r)\n",
    "        t_re = self.E_re(t)\n",
    "        t_im = self.E_im(t)\n",
    "        \n",
    "        # è®¡ç®—æ—‹è½¬åçš„å¤´å®ä½“: h * r\n",
    "        hr_re = h_re * r_re - h_im * r_im\n",
    "        hr_im = h_re * r_im + h_im * r_re\n",
    "        \n",
    "        # è·ç¦»ï¼š|hr - t|^2\n",
    "        score = torch.sum((hr_re - t_re)**2 + (hr_im - t_im)**2, dim=1)\n",
    "        return score\n",
    "\n",
    "    def get_query_embedding(self, h, r):\n",
    "        # è¿”å› h * r çš„å¤æ•°å‘é‡æ‹¼æ¥ [hr_re, hr_im]\n",
    "        h_re = self.E_re(h)\n",
    "        h_im = self.E_im(h)\n",
    "        r_re = self.R_re(r)\n",
    "        r_im = self.R_im(r)\n",
    "        hr_re = h_re * r_re - h_im * r_im\n",
    "        hr_im = h_re * r_im + h_im * r_re\n",
    "        return torch.cat([hr_re, hr_im], dim=1)  # [B, 2*dim]\n",
    "\n",
    "# ==================== ConvE ====================\n",
    "class ConvE(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, dim, embedding_dim=100, hidden_drop=0.3, feature_map_drop=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = dim\n",
    "        self.reshape = (1, 10, 10)  # å‡è®¾ dim=100 -> 10x10\n",
    "        self.feature_map_drop = feature_map_drop\n",
    "\n",
    "        # å®ä½“å’Œå…³ç³»åµŒå…¥\n",
    "        self.E = nn.Embedding(num_entities, embedding_dim)\n",
    "        self.R = nn.Embedding(num_relations, embedding_dim)\n",
    "\n",
    "        # å·ç§¯å±‚\n",
    "        self.conv1 = nn.Conv2d(1, 32, (3, 3), stride=1, padding=1)\n",
    "        self.bn0 = nn.BatchNorm2d(1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm1d(embedding_dim)\n",
    "        self.dropout0 = nn.Dropout2d(hidden_drop)\n",
    "        self.dropout1 = nn.Dropout2d(feature_map_drop)\n",
    "        self.fc = nn.Linear(32 * 10 * 10, embedding_dim)\n",
    "\n",
    "        # åˆå§‹åŒ–\n",
    "        nn.init.xavier_uniform_(self.E.weight)\n",
    "        nn.init.xavier_uniform_(self.R.weight)\n",
    "\n",
    "    def forward(self, h, r, t):\n",
    "        h_emb = self.E(h).view(-1, 1, 10, 10)  # [B, 1, 10, 10]\n",
    "        r_emb = self.R(r).view(-1, 1, 10, 10)\n",
    "        stacked = torch.cat([h_emb, r_emb], 2)  # [B, 1, 20, 10] â†’ ä¸å¯¹ï¼Œåº”æ‹¼åœ¨é€šé“ç»´\n",
    "\n",
    "        # æ­£ç¡®æ–¹å¼ï¼šåœ¨é€šé“ç»´æ‹¼æ¥ [B, 2, 10, 10]\n",
    "        stacked = torch.cat([h_emb, r_emb], dim=1)  # [B, 2, 10, 10]\n",
    "\n",
    "        # BatchNorm + Dropout\n",
    "        x = self.bn0(stacked)\n",
    "        x = self.conv1(x)  # [B, 32, 10, 10]\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = x.view(x.size(0), -1)  # [B, 32*10*10]\n",
    "        x = self.fc(x)  # [B, 100]\n",
    "        x = self.bn2(x)\n",
    "        x = torch.dropout(x, p=self.feature_map_drop, train=self.training)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # å¾—åˆ†ï¼šx ä¸ t çš„ç‚¹ç§¯\n",
    "        t_emb = self.E(t)\n",
    "        score = torch.sum(x * t_emb, dim=1)\n",
    "        return score\n",
    "\n",
    "    def get_query_embedding(self, h, r):\n",
    "        h_emb = self.E(h).view(-1, 1, 10, 10)\n",
    "        r_emb = self.R(r).view(-1, 1, 10, 10)\n",
    "        stacked = torch.cat([h_emb, r_emb], dim=1)\n",
    "        x = self.bn0(stacked)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        return x  # [B, 100]\n",
    "\n",
    "# ==================== è®­ç»ƒ & åŠ è½½ ====================\n",
    "def train_model(model, model_name, train_dataset, mapper, device):\n",
    "    if os.path.exists(TRAINED_MODEL_PATHS[model_name]):\n",
    "        print(f\"[{model_name}] å·²å­˜åœ¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè·³è¿‡è®­ç»ƒ\")\n",
    "        return\n",
    "    print(f\"[{model_name}] å¼€å§‹è®­ç»ƒ...\")\n",
    "    loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_DECAY_STEP, gamma=LR_DECAY_FACTOR)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        progress = tqdm(loader, desc=f\"{model_name} Epoch {epoch+1}\")\n",
    "        for h_list, r_list, t_list in progress:\n",
    "            h = torch.tensor([mapper.entity_to_id[h] for h in h_list], device=device)\n",
    "            r = torch.tensor([mapper.relation_to_id[r] for r in r_list], device=device)\n",
    "            t = torch.tensor([mapper.entity_to_id[t] for t in t_list], device=device)\n",
    "            ## ã€TODOã€‘è¿™é‡Œè¦ç¡®ä¿è´Ÿæ ·æœ¬å’ŒåŸæ¥çš„ä¸ä¸€æ ·ã€‚\n",
    "            neg_t = torch.randint(0, mapper.entity_count, (len(h), NEGATIVE_SAMPLES), device=device)             \n",
    "\n",
    "            pos_score = model(h, r, t)\n",
    "            neg_score = model(\n",
    "                h.unsqueeze(1).expand(-1, NEGATIVE_SAMPLES).reshape(-1),\n",
    "                r.unsqueeze(1).expand(-1, NEGATIVE_SAMPLES).reshape(-1),\n",
    "                neg_t.reshape(-1)\n",
    "            ).reshape(-1, NEGATIVE_SAMPLES)\n",
    "\n",
    "            loss = torch.mean(torch.relu(pos_score.unsqueeze(1) - neg_score + 1.0))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            progress.set_postfix(loss=loss.item())\n",
    "        scheduler.step()\n",
    "        print(f\"[{model_name}] Epoch {epoch+1} Loss: {epoch_loss / len(loader):.4f}\")\n",
    "\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'entity_count': mapper.entity_count,\n",
    "        'relation_count': mapper.relation_count,\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'entity_to_id': mapper.entity_to_id,\n",
    "        'relation_to_id': mapper.relation_to_id,\n",
    "    }, TRAINED_MODEL_PATHS[model_name])\n",
    "    print(f\"[{model_name}] æ¨¡å‹å·²ä¿å­˜\")\n",
    "\n",
    "\n",
    "def load_model(model_class, model_path, mapper, device):\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model = model_class(checkpoint['entity_count'], checkpoint['relation_count'], EMBEDDING_DIM)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, dataset, mapper, device, k_list=(1, 3, 10)):\n",
    "    print(\"ğŸ” å¼€å§‹åœ¨å¼€å‘é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ˆä»…å°¾å®ä½“é¢„æµ‹ï¼‰...\")\n",
    "    model.eval()\n",
    "    hits_at = {k: 0.0 for k in k_list}\n",
    "    mrr = 0.0\n",
    "    count = 0\n",
    "\n",
    "    entity_emb = model.E.weight.data.cpu().numpy()\n",
    "    index = faiss.IndexFlatL2(entity_emb.shape[1])\n",
    "    index.add(entity_emb)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for h, r, t in tqdm(dataset.triples, desc=\"Evaluating\"):\n",
    "            try:\n",
    "                h_id = torch.tensor([mapper.entity_to_id[h]], device=device)\n",
    "                r_id = torch.tensor([mapper.relation_to_id[r]], device=device)\n",
    "                t_id = mapper.entity_to_id[t]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            # åªåšå°¾å®ä½“é¢„æµ‹ (h, r, ?)\n",
    "            query = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "            _, indices = index.search(query, 1000)\n",
    "            pred_ids = indices[0]\n",
    "\n",
    "            # è¿‡æ»¤\n",
    "            filtered_tails = [tail for head, rel, tail in mapper.all_train_triples if head == h and rel == r and tail != t]\n",
    "            filter_ids = [mapper.entity_to_id[tail] for tail in filtered_tails if tail in mapper.entity_to_id]\n",
    "            for fid in filter_ids:\n",
    "                if fid in pred_ids:\n",
    "                    mask = pred_ids == fid\n",
    "                    pred_ids = np.concatenate([pred_ids[~mask], pred_ids[mask]])\n",
    "\n",
    "            rank = np.where(pred_ids == t_id)[0]\n",
    "            final_rank = rank[0] + 1 if len(rank) > 0 else 10000\n",
    "\n",
    "            for k in k_list:\n",
    "                if final_rank <= k:\n",
    "                    hits_at[k] += 1\n",
    "            mrr += 1.0 / final_rank\n",
    "            count += 1\n",
    "\n",
    "    for k in hits_at:\n",
    "        hits_at[k] /= count\n",
    "    mrr /= count\n",
    "\n",
    "    print(\"âœ… è¯„ä¼°å®Œæˆï¼\")\n",
    "    print(f\"ğŸ“Š HITS@1:  {hits_at[1]:.4f}\")\n",
    "    print(f\"ğŸ“Š HITS@3:  {hits_at[3]:.4f}\")\n",
    "    print(f\"ğŸ“Š HITS@10: {hits_at[10]:.4f}\")\n",
    "    print(f\"ğŸ“Š MRR:     {mrr:.4f}\")\n",
    "\n",
    "    return hits_at, mrr\n",
    "\n",
    "def evaluate_ensemble(models_with_weights, dev_dataset, mapper, device, k_list=(1, 3, 10), rrf_k=60):\n",
    "# def evaluate_ensemble_rrf(models_with_weights, dev_dataset, mapper, device, k_list=(1, 3, 10), rrf_k=60):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Reciprocal Rank Fusion (RRF) èåˆå¤šä¸ªæ¨¡å‹çš„æ’åºç»“æœ\n",
    "    ç‰¹åˆ«è¦æ±‚ï¼šè‹¥æ­£ç¡®ç­”æ¡ˆä¸åœ¨ Top10 å€™é€‰ä¸­ï¼Œåˆ™ MRR å¾—åˆ†ä¸º 0\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸš€ å¼€å§‹è¯„ä¼°èåˆæ¨¡å‹ (RRF èåˆ) - Top10 å¤–ç­”æ¡ˆå¾—åˆ†ä¸º 0\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    hits_at = {k: 0.0 for k in k_list}\n",
    "    mrr_scores = []  # æ¯ä¸ª query çš„ MRR å¾—åˆ†ï¼ˆå¯èƒ½ä¸º 0ï¼‰\n",
    "    count = 0\n",
    "\n",
    "    # è·å–å®ä½“åµŒå…¥ï¼ˆç”¨äº FAISS æ£€ç´¢ï¼‰\n",
    "    sample_model = next(iter(models_with_weights.values()))[0]\n",
    "    entity_emb = sample_model.E.weight.data.cpu().numpy()\n",
    "    index = faiss.IndexFlatL2(entity_emb.shape[1])\n",
    "    index.add(entity_emb)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for h, r, t in tqdm(dev_dataset.triples, desc=\"RRF Eval\"):\n",
    "            try:\n",
    "                h_id = torch.tensor([mapper.entity_to_id[h]], device=device)\n",
    "                r_id = torch.tensor([mapper.relation_to_id[r]], device=device)\n",
    "                t_id = mapper.entity_to_id[t]\n",
    "            except KeyError:\n",
    "                # è·³è¿‡æœªç™»å½•å®ä½“\n",
    "                continue\n",
    "\n",
    "            # ========== æ”¶é›†æ¯ä¸ªæ¨¡å‹çš„æ’åºå¾—åˆ†ï¼ˆRRFï¼‰==========\n",
    "            rrf_scores = np.zeros(mapper.entity_count)\n",
    "\n",
    "            for name, (model, weight) in models_with_weights.items():\n",
    "                q = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "                _, indices = index.search(q, 1000)  # æ£€ç´¢ top 1000\n",
    "                candidate_ids = indices[0]\n",
    "\n",
    "                # RRF å…¬å¼: score += weight / (k + rank)\n",
    "                for rank, idx in enumerate(candidate_ids):\n",
    "                    rrf_scores[idx] += weight / (rrf_k + rank + 1)\n",
    "\n",
    "            # ========== è¿‡æ»¤è®­ç»ƒé›†ä¸­å·²å­˜åœ¨çš„ä¸‰å…ƒç»„ï¼ˆé™¤å½“å‰å¤–ï¼‰==========\n",
    "            filtered_tails = [\n",
    "                tail for head, rel, tail in mapper.all_train_triples\n",
    "                if head == h and rel == r and tail != t\n",
    "            ]\n",
    "            for tail in filtered_tails:\n",
    "                if tail in mapper.entity_to_id:\n",
    "                    rrf_scores[mapper.entity_to_id[tail]] = -1e9\n",
    "\n",
    "            # ========== æ’åºï¼ˆé™åºï¼‰==========\n",
    "            ranked_indices = np.argsort(rrf_scores)[::-1]\n",
    "\n",
    "            # ========== è·å– Top10 é¢„æµ‹ç»“æœ ==========\n",
    "            top10_ids = ranked_indices[:10]\n",
    "            top10_entities = [mapper.id_to_entity[i] for i in top10_ids]\n",
    "\n",
    "            # ========== è®¡ç®—æŒ‡æ ‡ ==========\n",
    "            # æ£€æŸ¥çœŸå® tail æ˜¯å¦åœ¨ Top10\n",
    "            if t_id in top10_ids:\n",
    "                rank = np.where(ranked_indices == t_id)[0][0] + 1  # æ’åä» 1 å¼€å§‹\n",
    "                mrr_score = 1.0 / rank\n",
    "            else:\n",
    "                mrr_score = 0.0  # Top10 ä¸åŒ…å«ï¼Œå¾—åˆ†ä¸º 0\n",
    "\n",
    "            # æ›´æ–° Hits\n",
    "            for k in k_list:\n",
    "                if t_id in top10_ids[:k]:\n",
    "                    hits_at[k] += 1\n",
    "\n",
    "            mrr_scores.append(mrr_score)\n",
    "            count += 1\n",
    "\n",
    "    # ========== è®¡ç®—æœ€ç»ˆæŒ‡æ ‡ ==========\n",
    "    for k in hits_at:\n",
    "        hits_at[k] /= count\n",
    "    mrr = np.mean(mrr_scores) if mrr_scores else 0.0\n",
    "\n",
    "    print(\"âœ… RRF èåˆè¯„ä¼°å®Œæˆï¼\")\n",
    "    print(f\"ğŸ“Š RRF HITS@1:  {hits_at[1]:.4f}\")\n",
    "    print(f\"ğŸ“Š RRF HITS@3:  {hits_at[3]:.4f}\")\n",
    "    print(f\"ğŸ“Š RRF HITS@10: {hits_at[10]:.4f}\")\n",
    "    print(f\"ğŸ“Š RRF MRR:     {mrr:.4f}\")\n",
    "\n",
    "    return hits_at, mrr\n",
    "\n",
    "\n",
    "# ==================== èåˆé¢„æµ‹ ====================\n",
    "def predict_ensemble(models_with_weights, test_dataset, mapper, device, max_head_entities=None, rrf_k=60): \n",
    "# def predict_ensemble_rrf(models_with_weights, test_dataset, mapper, device, max_head_entities=None, rrf_k=60):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ RRF èåˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼ˆä¸ evaluate_ensemble_rrf ä¿æŒä¸€è‡´ï¼‰\n",
    "    è¾“å‡ºæ¯ä¸ª query çš„ Top10 é¢„æµ‹ç»“æœ\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” å¼€å§‹èåˆé¢„æµ‹ (RRF èåˆ + FAISS åŠ é€Ÿ) ...\")\n",
    "    results = []\n",
    "\n",
    "    # è·å–å®ä½“åµŒå…¥\n",
    "    sample_model = next(iter(models_with_weights.values()))[0]\n",
    "    entity_emb = sample_model.E.weight.data.cpu().numpy()\n",
    "    index = faiss.IndexFlatL2(entity_emb.shape[1])\n",
    "    index.add(entity_emb)\n",
    "\n",
    "    triples = test_dataset.triples\n",
    "    if max_head_entities:\n",
    "        triples = triples[:max_head_entities]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for h, r, _ in tqdm(triples, desc=\"RRF Predict\"):\n",
    "            try:\n",
    "                h_id = torch.tensor([mapper.entity_to_id[h]], device=device)\n",
    "                r_id = torch.tensor([mapper.relation_to_id[r]], device=device)\n",
    "            except KeyError:\n",
    "                # è‹¥å®ä½“æœªç™»å½•ï¼Œè¿”å› 10 ä¸ªè‡ªèº«ï¼ˆæˆ–éšæœºï¼‰\n",
    "                preds = [h] * 10\n",
    "                results.append('\\t'.join([h, r] + preds))\n",
    "                continue\n",
    "\n",
    "            # ========== RRF èåˆ ==========\n",
    "            rrf_scores = np.zeros(mapper.entity_count)\n",
    "            for name, (model, weight) in models_with_weights.items():\n",
    "                q = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "                _, indices = index.search(q, 1000)\n",
    "                candidate_ids = indices[0]\n",
    "                for rank, idx in enumerate(candidate_ids):\n",
    "                    rrf_scores[idx] += weight / (rrf_k + rank + 1)\n",
    "\n",
    "            # ========== æ’åºå¹¶å– Top10 ==========\n",
    "            ranked_indices = np.argsort(rrf_scores)[::-1]\n",
    "            top10_ids = ranked_indices[:10]\n",
    "            preds = [mapper.id_to_entity[i] for i in top10_ids]\n",
    "\n",
    "            results.append('\\t'.join([h, r] + preds))\n",
    "\n",
    "    # ========== ä¿å­˜ç»“æœ ==========\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE_PATH), exist_ok=True)\n",
    "    with open(OUTPUT_FILE_PATH, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(results) + '\\n')\n",
    "\n",
    "    print(f\"âœ… RRF èåˆç»“æœå·²ä¿å­˜è‡³: {OUTPUT_FILE_PATH}\")\n",
    "\n",
    "    # å‹ç¼©ä¸º zip\n",
    "    zip_path = OUTPUT_FILE_PATH.replace(\".tsv\", \"\") + f\"__{scheme_type}.zip\"\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        zf.write(OUTPUT_FILE_PATH, arcname=os.path.basename(OUTPUT_FILE_PATH))\n",
    "    print(f\"âœ… å·²å‹ç¼©ä¸º: {zip_path}\")\n",
    "\n",
    "\n",
    "# ==================== ä¸»å‡½æ•° ====================\n",
    "def main():\n",
    "    device = torch.device('mps') if torch.backends.mps.is_available() else \\\n",
    "             torch.device('cpu')\n",
    "    print(f\"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "\n",
    "    train_data = KnowledgeGraphDataset(TRAIN_FILE_PATH, max_lines=MAX_LINES, is_train=True)\n",
    "    dev_data = KnowledgeGraphDataset(DEV_FILE_PATH, is_test=False, is_train=False)\n",
    "    test_data = KnowledgeGraphDataset(TEST_FILE_PATH, is_test=True, is_train=False)\n",
    "\n",
    "    mapper = EntityRelationMapper()\n",
    "    mapper.build_mappings(train_data, dev_data, test_data)\n",
    "    print(f\"å®ä½“æ•°: {mapper.entity_count}, å…³ç³»æ•°: {mapper.relation_count}\")\n",
    "\n",
    "    model_classes = {\n",
    "        # 'TransE': TransE, ##ã€TODOã€‘å®é™…è·‘çš„æ—¶å€™è¦æ”¾å¼€çš„è¿™é‡Œã€‚\n",
    "        # 'TransH': TransH,\n",
    "        # 'TransD': TransD,\n",
    "        'RotatE': RotatE,\n",
    "        # 'ConvE': ConvE,\n",
    "    }\n",
    "\n",
    "    # è®­ç»ƒæ‰€æœ‰æ¨¡å‹\n",
    "    for name, Cls in model_classes.items():\n",
    "        model = Cls(mapper.entity_count, mapper.relation_count, EMBEDDING_DIM)\n",
    "        train_model(model, name, train_data, mapper, device)\n",
    "\n",
    "    # åŠ è½½å¹¶è¯„ä¼°æ¯ä¸ªæ¨¡å‹\n",
    "    loaded_models_with_weight = {}\n",
    "    for name, Cls in model_classes.items():\n",
    "        model = load_model(Cls, TRAINED_MODEL_PATHS[name], mapper, device)\n",
    "        loaded_models_with_weight[name] = (model, 1.0)\n",
    "        print(f\"\\nğŸ“ˆ æ­£åœ¨è¯„ä¼°æ¨¡å‹: {name}\")\n",
    "        ##ã€TODOã€‘å®é™…è·‘çš„æ—¶å€™è¦æ”¾å¼€çš„è¿™é‡Œã€‚\n",
    "        evaluate_model(model, dev_data, mapper, device) \n",
    "    \n",
    "    # ##ã€TODOã€‘å®é™…è·‘çš„æ—¶å€™è¦æ”¾å¼€çš„è¿™é‡Œã€‚\n",
    "    # # ğŸ”¥ è¯„ä¼°èåˆæ¨¡å‹\n",
    "    # evaluate_ensemble(loaded_models_with_weight, dev_data, mapper, device) \n",
    "\n",
    "    # ##ã€TODOã€‘å®é™…è·‘çš„æ—¶å€™è¦æ”¾å¼€çš„è¿™é‡Œã€‚\n",
    "    # # æ‰§è¡Œèåˆé¢„æµ‹\n",
    "    # predict_ensemble(loaded_models_with_weight, test_data, mapper, device, MAX_HEAD_ENTITIES)\n",
    "    # print(\"ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼èåˆé¢„æµ‹åŠè¯„ä¼°å·²å®Œæˆã€‚\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffbef93-a109-49a3-9899-8787016d7768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b1ea3-bf26-4610-9cfc-b8c1e536e19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068cd625-800e-49ee-a762-629a073d3fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b406ee-65d4-4e10-8eea-94433d171dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03549c5-be01-4383-86e5-8b505df85a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9dee0b-927c-4bdf-85d7-f95b8f4a5b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad49a7dc-f745-4095-b69a-fcde661c226b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e7af8-2c07-47aa-ac37-18862b37750d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e836562-e932-49a2-bb14-b6072ceb4436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523a966e-52a2-42ea-8bb5-eb1fb2059281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7399566d-9691-4110-9026-73b3607af6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "570ad541-d2c0-49cd-9e47-342de5656cb8",
   "metadata": {},
   "source": [
    "# ï¼ˆæš‚æ—¶ç”¨ä¸ä¸Šçš„éƒ¨åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896e0dc-2b93-45d2-bd2a-226000ea1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_rank_comparison(models_with_weights, sample_triples, mapper, device, rrf_k=60):\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–è‹¥å¹² sample query çš„æ’åå¯¹æ¯”\n",
    "    sample_triples: List[(h, r, t)]\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(len(sample_triples), 1, figsize=(10, 2 * len(sample_triples)))\n",
    "    if len(sample_triples) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    entity_emb = next(iter(models_with_weights.values()))[0].E.weight.data.cpu().numpy()\n",
    "    index = faiss.IndexFlatL2(entity_emb.shape[1])\n",
    "    index.add(entity_emb)\n",
    "\n",
    "    for idx, (h, r, t) in enumerate(sample_triples):\n",
    "        ax = axes[idx]\n",
    "        model_ranks = {}\n",
    "        rrf_scores = np.zeros(mapper.entity_count)\n",
    "\n",
    "        try:\n",
    "            h_id = torch.tensor([mapper.entity_to_id[h]], device=device)\n",
    "            r_id = torch.tensor([mapper.relation_to_id[r]], device=device)\n",
    "            t_id = mapper.entity_to_id[t]\n",
    "        except KeyError:\n",
    "            ax.set_title(f\"{h} --{r}--> {t} (UNK)\")\n",
    "            ax.text(0.5, 0.5, \"Entity Not in Vocabulary\", ha='center')\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name, (model, weight) in models_with_weights.items():\n",
    "                q = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "                _, indices = index.search(q, 1000)\n",
    "                rank = np.where(indices[0] == t_id)[0]\n",
    "                model_ranks[name] = rank[0] + 1 if len(rank) > 0 else 1000\n",
    "\n",
    "                # æ”¶é›† RRF å¾—åˆ†\n",
    "                for r, idx_id in enumerate(indices[0]):\n",
    "                    rrf_scores[idx_id] += weight / (rrf_k + r + 1)\n",
    "\n",
    "        # RRF æ’å\n",
    "        rrf_ranked = np.argsort(rrf_scores)[::-1]\n",
    "        rrf_rank = np.where(rrf_ranked == t_id)[0]\n",
    "        model_ranks['RRF'] = rrf_rank[0] + 1 if len(rrf_rank) > 0 else 1000\n",
    "\n",
    "        # ç»˜å›¾\n",
    "        names = list(model_ranks.keys())\n",
    "        ranks = list(model_ranks.values())\n",
    "        colors = ['skyblue', 'lightgreen', 'salmon']\n",
    "\n",
    "        ax.bar(names, ranks, color=colors[:len(names)], alpha=0.8)\n",
    "        ax.set_ylabel(\"Rank (â†“ better)\")\n",
    "        ax.set_title(f\"{h} --{r}--> {t} | Ranks: {dict(zip(names, ranks))}\")\n",
    "        ax.set_yscale('log')  # å¯¹æ•°åˆ»åº¦ï¼Œä¾¿äºè§‚å¯Ÿå·®å¼‚å¤§çš„æ’å\n",
    "        ax.axhline(y=10, color='r', linestyle='--', alpha=0.5, label=\"Top10\")\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"rrf_rank_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31994d68-fa0f-460a-b7c3-eebfecfd58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rrf_heatmap(models_with_weights, query_triple, mapper, device, rrf_k=60, top_k=20):\n",
    "    h, r, t = query_triple\n",
    "    h_id = torch.tensor([mapper.entity_to_id[h]], device=device)\n",
    "    r_id = torch.tensor([mapper.relation_to_id[r]], device=device)\n",
    "    t_id = mapper.entity_to_id[t]\n",
    "\n",
    "    entity_emb = next(iter(models_with_weights.values()))[0].E.weight.data.cpu().numpy()\n",
    "    index = faiss.IndexFlatL2(entity_emb.shape[1])\n",
    "    index.add(entity_emb)\n",
    "\n",
    "    # è·å–æ¯ä¸ªæ¨¡å‹çš„ Top 1000 æ’å\n",
    "    model_ranks = {}\n",
    "    candidate_set = set()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name, (model, _) in models_with_weights.items():\n",
    "            q = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "            _, indices = index.search(q, 1000)\n",
    "            model_ranks[name] = indices[0]\n",
    "            candidate_set.update(indices[0][:top_k])  # å–æ¯ä¸ªæ¨¡å‹å‰ top_k\n",
    "\n",
    "    # å¼ºåˆ¶åŒ…å«çœŸå® tail\n",
    "    candidate_set.add(t_id)\n",
    "\n",
    "    # è½¬ä¸ºåˆ—è¡¨ï¼Œå– top_k\n",
    "    candidates = sorted(candidate_set)[:top_k]\n",
    "    candidate_names = [mapper.id_to_entity[cid] for cid in candidates]\n",
    "\n",
    "    # æ„å»º RRF å¾—åˆ†çŸ©é˜µ\n",
    "    heatmap_data = []\n",
    "    for name in models_with_weights:\n",
    "        scores = []\n",
    "        for cid in candidates:\n",
    "            if cid in model_ranks[name]:\n",
    "                rank = np.where(model_ranks[name] == cid)[0][0] + 1\n",
    "                score = 1.0 / (rrf_k + rank)\n",
    "            else:\n",
    "                score = 0.0\n",
    "            scores.append(score)\n",
    "        heatmap_data.append(scores)\n",
    "\n",
    "    # ç»˜å›¾\n",
    "    import seaborn as sns\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt=\".4f\", xticklabels=candidate_names, yticklabels=list(models_with_weights.keys()), cmap=\"YlGnBu\")\n",
    "    plt.title(f\"RRF Score Contributions for Query: {h} --{r}--> {t}\")\n",
    "    plt.xlabel(\"Candidate Entities\")\n",
    "    plt.ylabel(\"Model\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"rrf_heatmap.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c6dd2b-bc3c-4368-9239-efdd9f04ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_mrr_improvement(models_with_weights, dev_dataset, mapper, device, rrf_k=60):\n",
    "    transd_mrrs, transh_mrrs, rrf_mrrs = [], [], []\n",
    "\n",
    "    entity_emb = next(iter(models_with_weights.values()))[0].E.weight.data.cpu().numpy()\n",
    "    index = faiss.IndexFlatL2(entity_emb.shape[1])\n",
    "    index.add(entity_emb)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for h, r, t in tqdm(dev_dataset.triples, desc=\"Analyzing MRR Improvement\"):\n",
    "            try:\n",
    "                h_id = torch.tensor([mapper.entity_to_id[h]], device=device)\n",
    "                r_id = torch.tensor([mapper.relation_to_id[r]], device=device)\n",
    "                t_id = mapper.entity_to_id[t]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„ MRRï¼ˆTop10 å¤–ä¸º 0ï¼‰\n",
    "            def get_mrr_at_10(ranks):\n",
    "                for r in ranks:\n",
    "                    if r <= 10:\n",
    "                        return 1.0 / r\n",
    "                return 0.0\n",
    "\n",
    "            # TransH\n",
    "            q_h = models_with_weights['TransH'][0].get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "            _, idx_h = index.search(q_h, 1000)\n",
    "            rank_h = np.where(idx_h[0] == t_id)[0]\n",
    "            rank_h = rank_h[0] + 1 if len(rank_h) > 0 else 1000\n",
    "            transh_mrr = 1.0 / rank_h if rank_h <= 10 else 0.0\n",
    "\n",
    "            # TransD\n",
    "            q_d = models_with_weights['TransD'][0].get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "            _, idx_d = index.search(q_d, 1000)\n",
    "            rank_d = np.where(idx_d[0] == t_id)[0]\n",
    "            rank_d = rank_d[0] + 1 if len(rank_d) > 0 else 1000\n",
    "            transd_mrr = 1.0 / rank_d if rank_d <= 10 else 0.0\n",
    "\n",
    "            # RRF\n",
    "            rrf_scores = np.zeros(mapper.entity_count)\n",
    "            for name, (model, w) in models_with_weights.items():\n",
    "                q = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "                _, indices = index.search(q, 1000)\n",
    "                for r, idx in enumerate(indices[0]):\n",
    "                    rrf_scores[idx] += w / (rrf_k + r + 1)\n",
    "            rrf_ranked = np.argsort(rrf_scores)[::-1]\n",
    "            rrf_rank = np.where(rrf_ranked == t_id)[0]\n",
    "            rrf_rank = rrf_rank[0] + 1 if len(rrf_rank) > 0 else 1000\n",
    "            rrf_mrr = 1.0 / rrf_rank if rrf_rank <= 10 else 0.0\n",
    "\n",
    "            transh_mrrs.append(transh_mrr)\n",
    "            transd_mrrs.append(transd_mrr)\n",
    "            rrf_mrrs.append(rrf_mrr)\n",
    "\n",
    "    # ç»˜å›¾\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    improvements = np.array(rrf_mrrs) - np.maximum(transh_mrrs, transd_mrrs)\n",
    "    plt.hist(improvements, bins=50, alpha=0.7, color='purple')\n",
    "    plt.axvline(x=0, color='red', linestyle='--', label=\"No change\")\n",
    "    plt.title(\"RRF MRR Improvement over Best Single Model\")\n",
    "    plt.xlabel(\"MRR Improvement\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig(\"mrr_improvement_hist.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"RRF æå‡çš„ query æ•°: {np.sum(improvements > 0)}\")\n",
    "    print(f\"RRF ä¸‹é™çš„ query æ•°: {np.sum(improvements < 0)}\")\n",
    "    print(f\"RRF å¹³å‡æå‡: {improvements.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94bf2f5-8477-43ad-a8b0-a2883def47cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02425c-644a-4726-8320-0e87dff6ee3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5f6995-b5eb-4288-a2fb-358bf7b9468f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57825d3-5d06-4998-b780-053219fc3a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a99db-2a74-4d7e-af10-7b59670ad331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d329aab-4099-49b6-a4d1-ed8b637caad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
