{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3771edd-8b12-4ee9-9d8a-be3c32bbe6a3",
   "metadata": {},
   "source": [
    "# 跑模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74374859-f1a6-457d-ad30-f696d8b7e0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 使用设备: mps\n",
      "加载数据: /Users/minkexiu/Downloads/GitHub/Tianchi_EcommerceKG_mac/originalData/OpenBG500/OpenBG500_train.tsv\n",
      "共加载 1242550 个三元组\n",
      "加载数据: /Users/minkexiu/Downloads/GitHub/Tianchi_EcommerceKG_mac/originalData/OpenBG500/OpenBG500_dev.tsv\n",
      "共加载 5000 个三元组\n",
      "加载数据: /Users/minkexiu/Downloads/GitHub/Tianchi_EcommerceKG_mac/originalData/OpenBG500/OpenBG500_test.tsv\n",
      "共加载 5000 个三元组\n",
      "实体数: 249747, 关系数: 500\n",
      "[TransE] 开始训练...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransE Epoch 1:  10%|█▏          | 490/4854 [00:20<02:34, 28.33it/s, loss=0.583]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import faiss  # 加速 Top-K 搜索\n",
    "\n",
    "# 设置随机种子\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "scheme_type = \"srs3_tryEHD\"\n",
    "\n",
    "# 数据路径（请根据你的实际路径修改）\n",
    "BASE_DIR = \"/Users/minkexiu/Downloads/GitHub/Tianchi_EcommerceKG_mac\"\n",
    "TRAIN_FILE_PATH = f\"{BASE_DIR}/originalData/OpenBG500/OpenBG500_train.tsv\"\n",
    "TEST_FILE_PATH = f\"{BASE_DIR}/originalData/OpenBG500/OpenBG500_test.tsv\"\n",
    "DEV_FILE_PATH = f\"{BASE_DIR}/originalData/OpenBG500/OpenBG500_dev.tsv\"\n",
    "OUTPUT_FILE_PATH = f\"{BASE_DIR}/preprocessedData/OpenBG500_test.tsv\" \n",
    "\n",
    "# 模型保存路径\n",
    "MODEL_DIR = f\"{BASE_DIR}/trained_models/{scheme_type}\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# 模型路径 \n",
    "TRAINED_MODEL_PATHS = {\n",
    "    'TransE': f\"{MODEL_DIR}/transE.pth\",\n",
    "    'TransH': f\"{MODEL_DIR}/transH.pth\",\n",
    "    'TransD': f\"{MODEL_DIR}/transD.pth\"\n",
    "}\n",
    "\n",
    "# 超参数\n",
    "EMBEDDING_DIM = 100\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 1 ##【TODO】这里可以修改多一点。\n",
    "BATCH_SIZE = 256\n",
    "NEGATIVE_SAMPLES = 10\n",
    "MAX_LINES = None\n",
    "MAX_HEAD_ENTITIES = None\n",
    "LR_DECAY_STEP = 5\n",
    "LR_DECAY_FACTOR = 0.1\n",
    "\n",
    "\n",
    "# ==================== 数据集 ====================\n",
    "class KnowledgeGraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path, is_test=False, max_lines=None, is_train=False):\n",
    "        self.triples = []\n",
    "        self.is_train = is_train\n",
    "        self._load_data(file_path, is_test, max_lines)\n",
    "\n",
    "    def _load_data(self, file_path, is_test, max_lines):\n",
    "        print(f\"加载数据: {file_path}\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            if max_lines:\n",
    "                lines = lines[:max_lines]\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 3:\n",
    "                    h, r, t = parts\n",
    "                    self.triples.append((h, r, t))\n",
    "                elif is_test and len(parts) == 2:\n",
    "                    h, r = parts\n",
    "                    self.triples.append((h, r, \"<UNK>\"))\n",
    "        print(f\"共加载 {len(self.triples)} 个三元组\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.triples[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    h_list, r_list, t_list = zip(*batch)\n",
    "    return list(h_list), list(r_list), list(t_list)\n",
    "\n",
    "\n",
    "# ==================== 映射器 ====================\n",
    "class EntityRelationMapper:\n",
    "    def __init__(self):\n",
    "        self.entity_to_id = {}\n",
    "        self.id_to_entity = {}\n",
    "        self.relation_to_id = {}\n",
    "        self.id_to_relation = {}\n",
    "        self.entity_count = 0\n",
    "        self.relation_count = 0\n",
    "        self.all_train_triples = []\n",
    "\n",
    "    def build_mappings(self, *datasets):\n",
    "        entities = set()\n",
    "        relations = set()\n",
    "        for dataset in datasets:\n",
    "            for h, r, t in dataset.triples:\n",
    "                entities.add(h)\n",
    "                entities.add(t)\n",
    "                relations.add(r)\n",
    "                if dataset.is_train:\n",
    "                    self.all_train_triples.append((h, r, t))\n",
    "\n",
    "        for e in sorted(entities):\n",
    "            self.entity_to_id[e] = self.entity_count\n",
    "            self.id_to_entity[self.entity_count] = e\n",
    "            self.entity_count += 1\n",
    "        for r in sorted(relations):\n",
    "            self.relation_to_id[r] = self.relation_count\n",
    "            self.id_to_relation[self.relation_count] = r\n",
    "            self.relation_count += 1\n",
    "\n",
    "\n",
    "# ==================== TransE （已优化）====================\n",
    "class TransE(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, dim):\n",
    "        super().__init__()\n",
    "        self.E = nn.Embedding(num_entities, dim)\n",
    "        self.R = nn.Embedding(num_relations, dim)\n",
    "        nn.init.xavier_uniform_(self.E.weight)\n",
    "        nn.init.xavier_uniform_(self.R.weight)\n",
    "\n",
    "    def forward(self, h, r, t):\n",
    "        return torch.norm(self.E(h) + self.R(r) - self.E(t), p=1, dim=1)\n",
    "\n",
    "    def get_query_embedding(self, h, r):\n",
    "        return self.E(h) + self.R(r)\n",
    "\n",
    "    def normalize_entities(self):\n",
    "        \"\"\"归一化实体嵌入\"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.E.weight.data.div_(torch.norm(self.E.weight.data, dim=1, keepdim=True) + 1e-9)\n",
    "\n",
    "\n",
    "# ==================== TransH （已修复）====================\n",
    "class TransH(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, dim):\n",
    "        super().__init__()\n",
    "        self.E = nn.Embedding(num_entities, dim)\n",
    "        self.R = nn.Embedding(num_relations, dim)      # 关系向量 d_r\n",
    "        self.W = nn.Embedding(num_relations, dim)    # 法向量 W (用于超平面)\n",
    "        nn.init.xavier_uniform_(self.E.weight)\n",
    "        nn.init.xavier_uniform_(self.R.weight)\n",
    "        nn.init.xavier_uniform_(self.W.weight)\n",
    "\n",
    "    def project(self, emb, w):  # 将实体投影到关系超平面上\n",
    "        # w: [B, dim], emb: [B, dim]\n",
    "        norm_w = torch.nn.functional.normalize(w, p=2, dim=1)  # 单位化法向量\n",
    "        scale = torch.sum(emb * norm_w, dim=1, keepdim=True)  # <e, w>\n",
    "        return emb - scale * norm_w  # e - <e, w> * w\n",
    "\n",
    "    def forward(self, h, r, t):\n",
    "        h_emb = self.E(h)  # [B, dim]\n",
    "        t_emb = self.E(t)\n",
    "        r_vec = self.R(r)\n",
    "        W = self.W(r)\n",
    "        h_proj = self.project(h_emb, W)\n",
    "        t_proj = self.project(t_emb, W)\n",
    "        return torch.norm(h_proj + r_vec - t_proj, p=1, dim=1)\n",
    "\n",
    "    def get_query_embedding(self, h, r):\n",
    "        h_emb = self.E(h)\n",
    "        r_vec = self.R(r)\n",
    "        W = self.W(r)\n",
    "        h_proj = self.project(h_emb, W)\n",
    "        return h_proj + r_vec  # 查询向量\n",
    "\n",
    "    def normalize_entities(self):\n",
    "        \"\"\"归一化实体嵌入，防止范数爆炸\"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.E.weight.data.div_(torch.norm(self.E.weight.data, dim=1, keepdim=True) + 1e-9)\n",
    "\n",
    "\n",
    "# ==================== TransD （已修复 + 优化）====================\n",
    "class TransD(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.E = nn.Embedding(num_entities, dim)\n",
    "        self.R = nn.Embedding(num_relations, dim)\n",
    "        self.E_proj = nn.Embedding(num_entities, dim)  # 实体投影向量\n",
    "        self.R_proj = nn.Embedding(num_relations, dim)  # 关系投影向量\n",
    "\n",
    "        # 初始化\n",
    "        nn.init.xavier_uniform_(self.E.weight)\n",
    "        nn.init.xavier_uniform_(self.R.weight)\n",
    "        nn.init.xavier_uniform_(self.E_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.R_proj.weight)\n",
    "\n",
    "    def project(self, e, e_proj, r_proj):\n",
    "        \"\"\"将实体 e 投影到由 e_proj 和 r_proj 定义的空间\"\"\"\n",
    "        # TransD: e_m = e_proj * r_proj^T * e\n",
    "        # 简化实现：e_m = e + (e_proj · r_proj) * e\n",
    "        # 更标准做法：使用外积投影，但常用简化为：e_m = e + (e_proj @ r_proj.T) 不可行，改为：\n",
    "        # 标准做法：e_m = W_r * e * W_e^T，但实现复杂，常用简化：\n",
    "        # 这里采用：h_proj = h + (h_proj_vec @ r_proj_vec.T) 不可行\n",
    "        # 改为标准实现：h_proj = h + (h_proj_vec · r_proj_vec) * h？不对\n",
    "\n",
    "        # 正确简化 TransD 投影：h_proj = h + (h_proj_vec @ r_proj_vec.T) × h？太复杂\n",
    "\n",
    "        # 更常见实现：使用双线性投影\n",
    "        # 参考：https://arxiv.org/abs/1509.05490\n",
    "        # 我们采用简化版本：h_proj = h + (h_proj_vec · r_proj_vec) * h？不对\n",
    "\n",
    "        # ✅ 标准实现（PyKE 等库）：\n",
    "        # h_proj = h + (h_proj_vec @ r_proj_vec.T) 不对\n",
    "        # 正确：h_proj = W_r * h * W_e^T → 太复杂\n",
    "\n",
    "        # ✅ 简化版本（广泛使用）：\n",
    "        # h_proj = h + (h_proj_vec ⊗ r_proj_vec) × h？也不对\n",
    "\n",
    "        # ✅ 实际常用实现（类似 TransR）：\n",
    "        # h_proj = h + (h_proj_vec · r_proj_vec) 是标量，不能直接加\n",
    "\n",
    "        # 🛠️ 修正：使用外积构造投影矩阵太慢，常用近似：\n",
    "        # h_proj = h + (h_proj_vec * r_proj_vec) * h？维度不对\n",
    "\n",
    "        # ✅ 正确简化（来自 OpenKE）：\n",
    "        # h_proj = h + (h_proj_vec @ r_proj_vec.T) 不可行\n",
    "\n",
    "        # 🚫 原始代码逻辑错误，我们改为 **标准 TransD 投影公式**：\n",
    "\n",
    "        # 正确公式：M_{r} = r_proj * h_proj^T + I\n",
    "        # h_proj = M_r @ h\n",
    "        # 但计算 M_r 是 [dim, dim]，太大\n",
    "\n",
    "        # ✅ 实用实现（来自 ConvE 论文实现）：\n",
    "        # h_proj = h + (h_proj_vec · r_proj_vec) * h？仍不对\n",
    "\n",
    "        # 🛠️ 改为 **正确但高效实现**（参考：https://github.com/thunlp/OpenKE/blob/OpenKE-PyTorch/models/TransD.py）\n",
    "\n",
    "        # 正确做法：\n",
    "        return e + torch.sum(e * e_proj, dim=1, keepdim=True) * r_proj\n",
    "        # 这是常见近似，表示：投影方向由 e_proj 和 r_proj 共同决定\n",
    "\n",
    "    def forward(self, h, r, t):\n",
    "        h_emb = self.project(self.E(h), self.E_proj(h), self.R_proj(r))\n",
    "        t_emb = self.project(self.E(t), self.E_proj(t), self.R_proj(r))\n",
    "        r_vec = self.R(r)\n",
    "        return torch.norm(h_emb + r_vec - t_emb, p=1, dim=1)\n",
    "\n",
    "    def get_query_embedding(self, h, r):\n",
    "        h_emb = self.project(self.E(h), self.E_proj(h), self.R_proj(r))\n",
    "        r_vec = self.R(r)\n",
    "        return h_emb + r_vec\n",
    "\n",
    "    def normalize_entities(self):\n",
    "        with torch.no_grad():\n",
    "            self.E.weight.data.div_(torch.norm(self.E.weight.data, dim=1, keepdim=True) + 1e-9)\n",
    "\n",
    "\n",
    "# ==================== 训练 & 加载 ====================\n",
    "def train_model(model, model_name, train_dataset, mapper, device):\n",
    "    if os.path.exists(TRAINED_MODEL_PATHS[model_name]) and not FORCE_RETRAIN:\n",
    "        print(f\"[{model_name}] 已存在训练好的模型，跳过训练\")\n",
    "        return\n",
    "    print(f\"[{model_name}] 开始训练...\")\n",
    "    loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_DECAY_STEP, gamma=LR_DECAY_FACTOR)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        progress = tqdm(loader, desc=f\"{model_name} Epoch {epoch+1}\")\n",
    "        for h_list, r_list, t_list in progress:\n",
    "            h = torch.tensor([mapper.entity_to_id[h] for h in h_list], device=device)\n",
    "            r = torch.tensor([mapper.relation_to_id[r] for r in r_list], device=device)\n",
    "            t = torch.tensor([mapper.entity_to_id[t] for t in t_list], device=device)\n",
    "\n",
    "            # ========== 生成负样本（确保不等于正样本）==========\n",
    "            neg_t = torch.randint(0, mapper.entity_count, (len(h), NEGATIVE_SAMPLES), device=device)\n",
    "            pos_t_expanded = t.unsqueeze(1).expand(-1, NEGATIVE_SAMPLES)  # [B, K]\n",
    "            mask = (neg_t == pos_t_expanded)\n",
    "            while mask.any():\n",
    "                neg_t[mask] = torch.randint(0, mapper.entity_count, (mask.sum(),), device=device)\n",
    "                mask = (neg_t == pos_t_expanded)  # 重新检查\n",
    "\n",
    "            # ========== 前向传播 ==========\n",
    "            pos_score = model(h, r, t)\n",
    "            neg_score = model(\n",
    "                h.unsqueeze(1).expand(-1, NEGATIVE_SAMPLES).reshape(-1),\n",
    "                r.unsqueeze(1).expand(-1, NEGATIVE_SAMPLES).reshape(-1),\n",
    "                neg_t.reshape(-1)\n",
    "            ).reshape(-1, NEGATIVE_SAMPLES)\n",
    "\n",
    "            # ========== 损失计算 ==========\n",
    "            loss = torch.mean(torch.relu(pos_score.unsqueeze(1) - neg_score + 1.0))\n",
    "\n",
    "            # ========== 反向传播 ==========\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # ========== 实体归一化 ==========\n",
    "            if hasattr(model, 'normalize_entities'):\n",
    "                model.normalize_entities()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            progress.set_postfix(loss=loss.item())\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"[{model_name}] Epoch {epoch+1} Loss: {epoch_loss / len(loader):.4f}\")\n",
    "\n",
    "    # ========== 保存模型 ==========\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'entity_count': mapper.entity_count,\n",
    "        'relation_count': mapper.relation_count,\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'entity_to_id': mapper.entity_to_id,\n",
    "        'relation_to_id': mapper.relation_to_id,\n",
    "    }, TRAINED_MODEL_PATHS[model_name])\n",
    "    print(f\"[{model_name}] 模型已保存\")\n",
    "\n",
    "\n",
    "def load_model(model_class, model_path, mapper, device):\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model = model_class(checkpoint['entity_count'], checkpoint['relation_count'], EMBEDDING_DIM)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, dataset, mapper, device, k_list=(1, 3, 10)):\n",
    "    print(\"🔍 开始在开发集上评估模型性能（仅尾实体预测）...\")\n",
    "    model.eval()\n",
    "    hits_at = {k: 0.0 for k in k_list}\n",
    "    mrr = 0.0\n",
    "    count = 0\n",
    "\n",
    "    entity_emb = model.E.weight.data.cpu().numpy()\n",
    "    index = faiss.IndexFlatL2(entity_emb.shape[1])\n",
    "    index.add(entity_emb)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for h, r, t in tqdm(dataset.triples, desc=\"Evaluating\"):\n",
    "            try:\n",
    "                h_id = torch.tensor([mapper.entity_to_id[h]], device=device)\n",
    "                r_id = torch.tensor([mapper.relation_to_id[r]], device=device)\n",
    "                t_id = mapper.entity_to_id[t]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            # 只做尾实体预测 (h, r, ?)\n",
    "            query = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "            _, indices = index.search(query, 1000)\n",
    "            pred_ids = indices[0]\n",
    "\n",
    "            # 过滤\n",
    "            filtered_tails = [tail for head, rel, tail in mapper.all_train_triples if head == h and rel == r and tail != t]\n",
    "            filter_ids = [mapper.entity_to_id[tail] for tail in filtered_tails if tail in mapper.entity_to_id]\n",
    "            for fid in filter_ids:\n",
    "                if fid in pred_ids:\n",
    "                    mask = pred_ids == fid\n",
    "                    pred_ids = np.concatenate([pred_ids[~mask], pred_ids[mask]])\n",
    "\n",
    "            rank = np.where(pred_ids == t_id)[0]\n",
    "            final_rank = rank[0] + 1 if len(rank) > 0 else 10000\n",
    "\n",
    "            for k in k_list:\n",
    "                if final_rank <= k:\n",
    "                    hits_at[k] += 1\n",
    "            mrr += 1.0 / final_rank\n",
    "            count += 1\n",
    "\n",
    "    for k in hits_at:\n",
    "        hits_at[k] /= count\n",
    "    mrr /= count\n",
    "\n",
    "    print(\"✅ 评估完成！\")\n",
    "    print(f\"📊 HITS@1:  {hits_at[1]:.4f}\")\n",
    "    print(f\"📊 HITS@3:  {hits_at[3]:.4f}\")\n",
    "    print(f\"📊 HITS@10: {hits_at[10]:.4f}\")\n",
    "    print(f\"📊 MRR:     {mrr:.4f}\")\n",
    "\n",
    "    return hits_at, mrr\n",
    "\n",
    "def evaluate_ensemble(models_with_weights, dev_dataset, mapper, device, k_list=(1, 3, 10), rrf_k=60):\n",
    "# def evaluate_ensemble_rrf(models_with_weights, dev_dataset, mapper, device, k_list=(1, 3, 10), rrf_k=60):\n",
    "    \"\"\"\n",
    "    使用 Reciprocal Rank Fusion (RRF) 融合多个模型的排序结果\n",
    "    特别要求：若正确答案不在 Top10 候选中，则 MRR 得分为 0\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🚀 开始评估融合模型 (RRF 融合) - Top10 外答案得分为 0\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    hits_at = {k: 0.0 for k in k_list}\n",
    "    mrr_scores = []  # 每个 query 的 MRR 得分（可能为 0）\n",
    "    count = 0\n",
    "\n",
    "    # 获取实体嵌入（用于 FAISS 检索）\n",
    "    sample_model = next(iter(models_with_weights.values()))[0]\n",
    "    entity_emb = sample_model.E.weight.data.cpu().numpy()\n",
    "    index = faiss.IndexFlatL2(entity_emb.shape[1])\n",
    "    index.add(entity_emb)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for h, r, t in tqdm(dev_dataset.triples, desc=\"RRF Eval\"):\n",
    "            try:\n",
    "                h_id = torch.tensor([mapper.entity_to_id[h]], device=device)\n",
    "                r_id = torch.tensor([mapper.relation_to_id[r]], device=device)\n",
    "                t_id = mapper.entity_to_id[t]\n",
    "            except KeyError:\n",
    "                # 跳过未登录实体\n",
    "                continue\n",
    "\n",
    "            # ========== 收集每个模型的排序得分（RRF）==========\n",
    "            rrf_scores = np.zeros(mapper.entity_count)\n",
    "\n",
    "            for name, (model, weight) in models_with_weights.items():\n",
    "                q = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "                _, indices = index.search(q, 1000)  # 检索 top 1000\n",
    "                candidate_ids = indices[0]\n",
    "\n",
    "                # RRF 公式: score += weight / (k + rank)\n",
    "                for rank, idx in enumerate(candidate_ids):\n",
    "                    rrf_scores[idx] += weight / (rrf_k + rank + 1)\n",
    "\n",
    "            # ========== 过滤训练集中已存在的三元组（除当前外）==========\n",
    "            filtered_tails = [\n",
    "                tail for head, rel, tail in mapper.all_train_triples\n",
    "                if head == h and rel == r and tail != t\n",
    "            ]\n",
    "            for tail in filtered_tails:\n",
    "                if tail in mapper.entity_to_id:\n",
    "                    rrf_scores[mapper.entity_to_id[tail]] = -1e9\n",
    "\n",
    "            # ========== 排序（降序）==========\n",
    "            ranked_indices = np.argsort(rrf_scores)[::-1]\n",
    "\n",
    "            # ========== 获取 Top10 预测结果 ==========\n",
    "            top10_ids = ranked_indices[:10]\n",
    "            top10_entities = [mapper.id_to_entity[i] for i in top10_ids]\n",
    "\n",
    "            # ========== 计算指标 ==========\n",
    "            # 检查真实 tail 是否在 Top10\n",
    "            if t_id in top10_ids:\n",
    "                rank = np.where(ranked_indices == t_id)[0][0] + 1  # 排名从 1 开始\n",
    "                mrr_score = 1.0 / rank\n",
    "            else:\n",
    "                mrr_score = 0.0  # Top10 不包含，得分为 0\n",
    "\n",
    "            # 更新 Hits\n",
    "            for k in k_list:\n",
    "                if t_id in top10_ids[:k]:\n",
    "                    hits_at[k] += 1\n",
    "\n",
    "            mrr_scores.append(mrr_score)\n",
    "            count += 1\n",
    "\n",
    "    # ========== 计算最终指标 ==========\n",
    "    for k in hits_at:\n",
    "        hits_at[k] /= count\n",
    "    mrr = np.mean(mrr_scores) if mrr_scores else 0.0\n",
    "\n",
    "    print(\"✅ RRF 融合评估完成！\")\n",
    "    print(f\"📊 RRF HITS@1:  {hits_at[1]:.4f}\")\n",
    "    print(f\"📊 RRF HITS@3:  {hits_at[3]:.4f}\")\n",
    "    print(f\"📊 RRF HITS@10: {hits_at[10]:.4f}\")\n",
    "    print(f\"📊 RRF MRR:     {mrr:.4f}\")\n",
    "\n",
    "    return hits_at, mrr\n",
    "\n",
    "\n",
    "# ==================== 融合预测 ====================\n",
    "def predict_ensemble(models_with_weights, test_dataset, mapper, device, max_head_entities=None, rrf_k=60): \n",
    "# def predict_ensemble_rrf(models_with_weights, test_dataset, mapper, device, max_head_entities=None, rrf_k=60):\n",
    "    \"\"\"\n",
    "    使用 RRF 融合多个模型的预测结果（与 evaluate_ensemble_rrf 保持一致）\n",
    "    输出每个 query 的 Top10 预测结果\n",
    "    \"\"\"\n",
    "    print(\"🔍 开始融合预测 (RRF 融合 + FAISS 加速) ...\")\n",
    "    results = []\n",
    "\n",
    "    # 获取实体嵌入\n",
    "    sample_model = next(iter(models_with_weights.values()))[0]\n",
    "    entity_emb = sample_model.E.weight.data.cpu().numpy()\n",
    "    index = faiss.IndexFlatL2(entity_emb.shape[1])\n",
    "    index.add(entity_emb)\n",
    "\n",
    "    triples = test_dataset.triples\n",
    "    if max_head_entities:\n",
    "        triples = triples[:max_head_entities]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for h, r, _ in tqdm(triples, desc=\"RRF Predict\"):\n",
    "            try:\n",
    "                h_id = torch.tensor([mapper.entity_to_id[h]], device=device)\n",
    "                r_id = torch.tensor([mapper.relation_to_id[r]], device=device)\n",
    "            except KeyError:\n",
    "                # 若实体未登录，返回 10 个自身（或随机）\n",
    "                preds = [h] * 10\n",
    "                results.append('\\t'.join([h, r] + preds))\n",
    "                continue\n",
    "\n",
    "            # ========== RRF 融合 ==========\n",
    "            rrf_scores = np.zeros(mapper.entity_count)\n",
    "            for name, (model, weight) in models_with_weights.items():\n",
    "                q = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "                _, indices = index.search(q, 1000)\n",
    "                candidate_ids = indices[0]\n",
    "                for rank, idx in enumerate(candidate_ids):\n",
    "                    rrf_scores[idx] += weight / (rrf_k + rank + 1)\n",
    "\n",
    "            # ========== 排序并取 Top10 ==========\n",
    "            ranked_indices = np.argsort(rrf_scores)[::-1]\n",
    "            top10_ids = ranked_indices[:10]\n",
    "            preds = [mapper.id_to_entity[i] for i in top10_ids]\n",
    "\n",
    "            results.append('\\t'.join([h, r] + preds))\n",
    "\n",
    "    # ========== 保存结果 ==========\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE_PATH), exist_ok=True)\n",
    "    with open(OUTPUT_FILE_PATH, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(results) + '\\n')\n",
    "\n",
    "    print(f\"✅ RRF 融合结果已保存至: {OUTPUT_FILE_PATH}\")\n",
    "\n",
    "    # 压缩为 zip\n",
    "    zip_path = OUTPUT_FILE_PATH.replace(\".tsv\", \"\") + f\"__{scheme_type}.zip\"\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        zf.write(OUTPUT_FILE_PATH, arcname=os.path.basename(OUTPUT_FILE_PATH))\n",
    "    print(f\"✅ 已压缩为: {zip_path}\")\n",
    "\n",
    "\n",
    "# ==================== 主函数 ====================\n",
    "def main():\n",
    "    device = torch.device('mps') if torch.backends.mps.is_available() else \\\n",
    "             torch.device('cpu')\n",
    "    print(f\"🚀 使用设备: {device}\")\n",
    "\n",
    "    train_data = KnowledgeGraphDataset(TRAIN_FILE_PATH, max_lines=MAX_LINES, is_train=True)\n",
    "    dev_data = KnowledgeGraphDataset(DEV_FILE_PATH, is_test=False, is_train=False)\n",
    "    test_data = KnowledgeGraphDataset(TEST_FILE_PATH, is_test=True, is_train=False)\n",
    "\n",
    "    mapper = EntityRelationMapper()\n",
    "    mapper.build_mappings(train_data, dev_data, test_data)\n",
    "    print(f\"实体数: {mapper.entity_count}, 关系数: {mapper.relation_count}\")\n",
    "\n",
    "    model_classes = {\n",
    "        'TransE': TransE, ##【TODO】实际跑的时候要放开的这里。\n",
    "        'TransH': TransH,\n",
    "        'TransD': TransD,\n",
    "    }\n",
    "\n",
    "    # 训练所有模型\n",
    "    for name, Cls in model_classes.items():\n",
    "        model = Cls(mapper.entity_count, mapper.relation_count, EMBEDDING_DIM)\n",
    "        train_model(model, name, train_data, mapper, device)\n",
    "\n",
    "    # 加载并评估每个模型\n",
    "    loaded_models_with_weight = {}\n",
    "    for name, Cls in model_classes.items():\n",
    "        model = load_model(Cls, TRAINED_MODEL_PATHS[name], mapper, device)\n",
    "        loaded_models_with_weight[name] = (model, 1.0)\n",
    "        print(f\"\\n📈 正在评估模型: {name}\")\n",
    "        ##【TODO】实际跑的时候要放开的这里。\n",
    "        evaluate_model(model, dev_data, mapper, device) \n",
    "    \n",
    "    # ##【TODO】实际跑的时候要放开的这里。\n",
    "    # # 🔥 评估融合模型\n",
    "    # evaluate_ensemble(loaded_models_with_weight, dev_data, mapper, device) \n",
    "\n",
    "    # ##【TODO】实际跑的时候要放开的这里。\n",
    "    # # 执行融合预测\n",
    "    # predict_ensemble(loaded_models_with_weight, test_data, mapper, device, MAX_HEAD_ENTITIES)\n",
    "    # print(\"🎉 所有任务完成！融合预测及评估已完成。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffbef93-a109-49a3-9899-8787016d7768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b1ea3-bf26-4610-9cfc-b8c1e536e19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf0e6e8-3eb3-4cea-a1f7-ef102fc34f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d03a8e-fbd1-4c03-a303-81ce769682aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7beb1df-3e83-457d-82b0-a04495f5befb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1637335-ab9d-4095-97c3-e31ee9a508e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912717ac-3d65-48ba-be05-25e1b7e99cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "068cd625-800e-49ee-a762-629a073d3fe5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise Exception(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b406ee-65d4-4e10-8eea-94433d171dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03549c5-be01-4383-86e5-8b505df85a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9dee0b-927c-4bdf-85d7-f95b8f4a5b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad49a7dc-f745-4095-b69a-fcde661c226b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e7af8-2c07-47aa-ac37-18862b37750d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e836562-e932-49a2-bb14-b6072ceb4436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523a966e-52a2-42ea-8bb5-eb1fb2059281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7399566d-9691-4110-9026-73b3607af6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "570ad541-d2c0-49cd-9e47-342de5656cb8",
   "metadata": {},
   "source": [
    "# （暂时用不上的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896e0dc-2b93-45d2-bd2a-226000ea1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_rank_comparison(models_with_weights, sample_triples, mapper, device, rrf_k=60):\n",
    "    \"\"\"\n",
    "    可视化若干 sample query 的排名对比\n",
    "    sample_triples: List[(h, r, t)]\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(len(sample_triples), 1, figsize=(10, 2 * len(sample_triples)))\n",
    "    if len(sample_triples) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    entity_emb = next(iter(models_with_weights.values()))[0].E.weight.data.cpu().numpy()\n",
    "    index = faiss.IndexFlatL2(entity_emb.shape[1])\n",
    "    index.add(entity_emb)\n",
    "\n",
    "    for idx, (h, r, t) in enumerate(sample_triples):\n",
    "        ax = axes[idx]\n",
    "        model_ranks = {}\n",
    "        rrf_scores = np.zeros(mapper.entity_count)\n",
    "\n",
    "        try:\n",
    "            h_id = torch.tensor([mapper.entity_to_id[h]], device=device)\n",
    "            r_id = torch.tensor([mapper.relation_to_id[r]], device=device)\n",
    "            t_id = mapper.entity_to_id[t]\n",
    "        except KeyError:\n",
    "            ax.set_title(f\"{h} --{r}--> {t} (UNK)\")\n",
    "            ax.text(0.5, 0.5, \"Entity Not in Vocabulary\", ha='center')\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name, (model, weight) in models_with_weights.items():\n",
    "                q = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "                _, indices = index.search(q, 1000)\n",
    "                rank = np.where(indices[0] == t_id)[0]\n",
    "                model_ranks[name] = rank[0] + 1 if len(rank) > 0 else 1000\n",
    "\n",
    "                # 收集 RRF 得分\n",
    "                for r, idx_id in enumerate(indices[0]):\n",
    "                    rrf_scores[idx_id] += weight / (rrf_k + r + 1)\n",
    "\n",
    "        # RRF 排名\n",
    "        rrf_ranked = np.argsort(rrf_scores)[::-1]\n",
    "        rrf_rank = np.where(rrf_ranked == t_id)[0]\n",
    "        model_ranks['RRF'] = rrf_rank[0] + 1 if len(rrf_rank) > 0 else 1000\n",
    "\n",
    "        # 绘图\n",
    "        names = list(model_ranks.keys())\n",
    "        ranks = list(model_ranks.values())\n",
    "        colors = ['skyblue', 'lightgreen', 'salmon']\n",
    "\n",
    "        ax.bar(names, ranks, color=colors[:len(names)], alpha=0.8)\n",
    "        ax.set_ylabel(\"Rank (↓ better)\")\n",
    "        ax.set_title(f\"{h} --{r}--> {t} | Ranks: {dict(zip(names, ranks))}\")\n",
    "        ax.set_yscale('log')  # 对数刻度，便于观察差异大的排名\n",
    "        ax.axhline(y=10, color='r', linestyle='--', alpha=0.5, label=\"Top10\")\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"rrf_rank_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31994d68-fa0f-460a-b7c3-eebfecfd58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rrf_heatmap(models_with_weights, query_triple, mapper, device, rrf_k=60, top_k=20):\n",
    "    h, r, t = query_triple\n",
    "    h_id = torch.tensor([mapper.entity_to_id[h]], device=device)\n",
    "    r_id = torch.tensor([mapper.relation_to_id[r]], device=device)\n",
    "    t_id = mapper.entity_to_id[t]\n",
    "\n",
    "    entity_emb = next(iter(models_with_weights.values()))[0].E.weight.data.cpu().numpy()\n",
    "    index = faiss.IndexFlatL2(entity_emb.shape[1])\n",
    "    index.add(entity_emb)\n",
    "\n",
    "    # 获取每个模型的 Top 1000 排名\n",
    "    model_ranks = {}\n",
    "    candidate_set = set()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name, (model, _) in models_with_weights.items():\n",
    "            q = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "            _, indices = index.search(q, 1000)\n",
    "            model_ranks[name] = indices[0]\n",
    "            candidate_set.update(indices[0][:top_k])  # 取每个模型前 top_k\n",
    "\n",
    "    # 强制包含真实 tail\n",
    "    candidate_set.add(t_id)\n",
    "\n",
    "    # 转为列表，取 top_k\n",
    "    candidates = sorted(candidate_set)[:top_k]\n",
    "    candidate_names = [mapper.id_to_entity[cid] for cid in candidates]\n",
    "\n",
    "    # 构建 RRF 得分矩阵\n",
    "    heatmap_data = []\n",
    "    for name in models_with_weights:\n",
    "        scores = []\n",
    "        for cid in candidates:\n",
    "            if cid in model_ranks[name]:\n",
    "                rank = np.where(model_ranks[name] == cid)[0][0] + 1\n",
    "                score = 1.0 / (rrf_k + rank)\n",
    "            else:\n",
    "                score = 0.0\n",
    "            scores.append(score)\n",
    "        heatmap_data.append(scores)\n",
    "\n",
    "    # 绘图\n",
    "    import seaborn as sns\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt=\".4f\", xticklabels=candidate_names, yticklabels=list(models_with_weights.keys()), cmap=\"YlGnBu\")\n",
    "    plt.title(f\"RRF Score Contributions for Query: {h} --{r}--> {t}\")\n",
    "    plt.xlabel(\"Candidate Entities\")\n",
    "    plt.ylabel(\"Model\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"rrf_heatmap.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c6dd2b-bc3c-4368-9239-efdd9f04ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_mrr_improvement(models_with_weights, dev_dataset, mapper, device, rrf_k=60):\n",
    "    transd_mrrs, transh_mrrs, rrf_mrrs = [], [], []\n",
    "\n",
    "    entity_emb = next(iter(models_with_weights.values()))[0].E.weight.data.cpu().numpy()\n",
    "    index = faiss.IndexFlatL2(entity_emb.shape[1])\n",
    "    index.add(entity_emb)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for h, r, t in tqdm(dev_dataset.triples, desc=\"Analyzing MRR Improvement\"):\n",
    "            try:\n",
    "                h_id = torch.tensor([mapper.entity_to_id[h]], device=device)\n",
    "                r_id = torch.tensor([mapper.relation_to_id[r]], device=device)\n",
    "                t_id = mapper.entity_to_id[t]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            # 计算每个模型的 MRR（Top10 外为 0）\n",
    "            def get_mrr_at_10(ranks):\n",
    "                for r in ranks:\n",
    "                    if r <= 10:\n",
    "                        return 1.0 / r\n",
    "                return 0.0\n",
    "\n",
    "            # TransH\n",
    "            q_h = models_with_weights['TransH'][0].get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "            _, idx_h = index.search(q_h, 1000)\n",
    "            rank_h = np.where(idx_h[0] == t_id)[0]\n",
    "            rank_h = rank_h[0] + 1 if len(rank_h) > 0 else 1000\n",
    "            transh_mrr = 1.0 / rank_h if rank_h <= 10 else 0.0\n",
    "\n",
    "            # TransD\n",
    "            q_d = models_with_weights['TransD'][0].get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "            _, idx_d = index.search(q_d, 1000)\n",
    "            rank_d = np.where(idx_d[0] == t_id)[0]\n",
    "            rank_d = rank_d[0] + 1 if len(rank_d) > 0 else 1000\n",
    "            transd_mrr = 1.0 / rank_d if rank_d <= 10 else 0.0\n",
    "\n",
    "            # RRF\n",
    "            rrf_scores = np.zeros(mapper.entity_count)\n",
    "            for name, (model, w) in models_with_weights.items():\n",
    "                q = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "                _, indices = index.search(q, 1000)\n",
    "                for r, idx in enumerate(indices[0]):\n",
    "                    rrf_scores[idx] += w / (rrf_k + r + 1)\n",
    "            rrf_ranked = np.argsort(rrf_scores)[::-1]\n",
    "            rrf_rank = np.where(rrf_ranked == t_id)[0]\n",
    "            rrf_rank = rrf_rank[0] + 1 if len(rrf_rank) > 0 else 1000\n",
    "            rrf_mrr = 1.0 / rrf_rank if rrf_rank <= 10 else 0.0\n",
    "\n",
    "            transh_mrrs.append(transh_mrr)\n",
    "            transd_mrrs.append(transd_mrr)\n",
    "            rrf_mrrs.append(rrf_mrr)\n",
    "\n",
    "    # 绘图\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    improvements = np.array(rrf_mrrs) - np.maximum(transh_mrrs, transd_mrrs)\n",
    "    plt.hist(improvements, bins=50, alpha=0.7, color='purple')\n",
    "    plt.axvline(x=0, color='red', linestyle='--', label=\"No change\")\n",
    "    plt.title(\"RRF MRR Improvement over Best Single Model\")\n",
    "    plt.xlabel(\"MRR Improvement\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig(\"mrr_improvement_hist.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"RRF 提升的 query 数: {np.sum(improvements > 0)}\")\n",
    "    print(f\"RRF 下降的 query 数: {np.sum(improvements < 0)}\")\n",
    "    print(f\"RRF 平均提升: {improvements.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94bf2f5-8477-43ad-a8b0-a2883def47cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02425c-644a-4726-8320-0e87dff6ee3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5f6995-b5eb-4288-a2fb-358bf7b9468f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57825d3-5d06-4998-b780-053219fc3a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a99db-2a74-4d7e-af10-7b59670ad331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d329aab-4099-49b6-a4d1-ed8b637caad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
