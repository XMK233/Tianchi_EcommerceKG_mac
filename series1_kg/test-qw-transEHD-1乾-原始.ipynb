{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74374859-f1a6-457d-ad30-f696d8b7e0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 使用设备: mps\n",
      "加载数据: /Users/minkexiu/Downloads/GitHub/Tianchi_EcommerceKG_mac/originalData/OpenBG500/OpenBG500_train.tsv\n",
      "共加载 1242550 个三元组\n",
      "加载数据: /Users/minkexiu/Downloads/GitHub/Tianchi_EcommerceKG_mac/originalData/OpenBG500/OpenBG500_dev.tsv\n",
      "共加载 5000 个三元组\n",
      "加载数据: /Users/minkexiu/Downloads/GitHub/Tianchi_EcommerceKG_mac/originalData/OpenBG500/OpenBG500_test.tsv\n",
      "共加载 5000 个三元组\n",
      "实体数: 249747, 关系数: 500\n",
      "[TransE] 已存在训练好的模型，跳过训练\n",
      "[TransH] 已存在训练好的模型，跳过训练\n",
      "[TransD] 已存在训练好的模型，跳过训练\n",
      "\n",
      "📈 正在评估模型: TransE\n",
      "🔍 开始在开发集上评估模型性能（仅尾实体预测）...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████| 5000/5000 [01:31<00:00, 54.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 评估完成！\n",
      "📊 HITS@1:  0.1798\n",
      "📊 HITS@3:  0.3836\n",
      "📊 HITS@10: 0.6298\n",
      "📊 MRR:     0.3252\n",
      "\n",
      "📈 正在评估模型: TransH\n",
      "🔍 开始在开发集上评估模型性能（仅尾实体预测）...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████| 5000/5000 [01:33<00:00, 53.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 评估完成！\n",
      "📊 HITS@1:  0.1122\n",
      "📊 HITS@3:  0.3262\n",
      "📊 HITS@10: 0.5490\n",
      "📊 MRR:     0.2551\n",
      "\n",
      "📈 正在评估模型: TransD\n",
      "🔍 开始在开发集上评估模型性能（仅尾实体预测）...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████| 5000/5000 [01:33<00:00, 53.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 评估完成！\n",
      "📊 HITS@1:  0.1742\n",
      "📊 HITS@3:  0.3316\n",
      "📊 HITS@10: 0.5306\n",
      "📊 MRR:     0.2906\n",
      "\n",
      "==================================================\n",
      "🚀 开始评估融合模型在开发集上的性能\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble Eval: 100%|████████████████████████| 5000/5000 [01:40<00:00, 49.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 融合模型评估完成！\n",
      "📊 Ensemble HITS@1:  0.0418\n",
      "📊 Ensemble HITS@3:  0.0958\n",
      "📊 Ensemble HITS@10: 0.1930\n",
      "📊 Ensemble MRR:     0.0919\n",
      "🔍 开始融合预测 (加权融合 + FAISS 加速) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble Predict: 100%|████████████████████| 5000/5000 [00:20<00:00, 247.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 融合结果已保存至: /Users/minkexiu/Downloads/GitHub/Tianchi_EcommerceKG_mac/preprocessedData/OpenBG500_test__ensemble_faiss.tsv\n",
      "✅ 已压缩为: /Users/minkexiu/Downloads/GitHub/Tianchi_EcommerceKG_mac/preprocessedData/OpenBG500_test__ensemble_faiss.zip\n",
      "🎉 所有任务完成！融合预测及评估已完成。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import faiss  # 加速 Top-K 搜索\n",
    "\n",
    "# 设置随机种子\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "scheme_type = \"ensemble_faiss\"\n",
    "\n",
    "# 数据路径（请根据你的实际路径修改）\n",
    "BASE_DIR = \"/Users/minkexiu/Downloads/GitHub/Tianchi_EcommerceKG_mac\"\n",
    "TRAIN_FILE_PATH = f\"{BASE_DIR}/originalData/OpenBG500/OpenBG500_train.tsv\"\n",
    "TEST_FILE_PATH = f\"{BASE_DIR}/originalData/OpenBG500/OpenBG500_test.tsv\"\n",
    "DEV_FILE_PATH = f\"{BASE_DIR}/originalData/OpenBG500/OpenBG500_dev.tsv\"\n",
    "OUTPUT_FILE_PATH = f\"{BASE_DIR}/preprocessedData/OpenBG500_test__{scheme_type}.tsv\"\n",
    "\n",
    "# 模型保存路径\n",
    "MODEL_DIR = f\"{BASE_DIR}/trained_model\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# 模型路径\n",
    "TRAINED_MODEL_PATHS = {\n",
    "    'TransE': f\"{MODEL_DIR}/trained_model__transE.pth\",\n",
    "    'TransH': f\"{MODEL_DIR}/trained_model__transH.pth\",\n",
    "    'TransD': f\"{MODEL_DIR}/trained_model__transD.pth\"\n",
    "}\n",
    "\n",
    "# 超参数\n",
    "EMBEDDING_DIM = 100\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 256\n",
    "NEGATIVE_SAMPLES = 10\n",
    "MAX_LINES = None\n",
    "MAX_HEAD_ENTITIES = None\n",
    "LR_DECAY_STEP = 5\n",
    "LR_DECAY_FACTOR = 0.1\n",
    "\n",
    "\n",
    "# ==================== 数据集 ====================\n",
    "class KnowledgeGraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path, is_test=False, max_lines=None, is_train=False):\n",
    "        self.triples = []\n",
    "        self.is_train = is_train\n",
    "        self._load_data(file_path, is_test, max_lines)\n",
    "\n",
    "    def _load_data(self, file_path, is_test, max_lines):\n",
    "        print(f\"加载数据: {file_path}\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            if max_lines:\n",
    "                lines = lines[:max_lines]\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 3:\n",
    "                    h, r, t = parts\n",
    "                    self.triples.append((h, r, t))\n",
    "                elif is_test and len(parts) == 2:\n",
    "                    h, r = parts\n",
    "                    self.triples.append((h, r, \"<UNK>\"))\n",
    "        print(f\"共加载 {len(self.triples)} 个三元组\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.triples[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    h_list, r_list, t_list = zip(*batch)\n",
    "    return list(h_list), list(r_list), list(t_list)\n",
    "\n",
    "\n",
    "# ==================== 映射器 ====================\n",
    "class EntityRelationMapper:\n",
    "    def __init__(self):\n",
    "        self.entity_to_id = {}\n",
    "        self.id_to_entity = {}\n",
    "        self.relation_to_id = {}\n",
    "        self.id_to_relation = {}\n",
    "        self.entity_count = 0\n",
    "        self.relation_count = 0\n",
    "        self.all_train_triples = []\n",
    "\n",
    "    def build_mappings(self, *datasets):\n",
    "        entities = set()\n",
    "        relations = set()\n",
    "        for dataset in datasets:\n",
    "            for h, r, t in dataset.triples:\n",
    "                entities.add(h)\n",
    "                entities.add(t)\n",
    "                relations.add(r)\n",
    "                if dataset.is_train:\n",
    "                    self.all_train_triples.append((h, r, t))\n",
    "\n",
    "        for e in sorted(entities):\n",
    "            self.entity_to_id[e] = self.entity_count\n",
    "            self.id_to_entity[self.entity_count] = e\n",
    "            self.entity_count += 1\n",
    "        for r in sorted(relations):\n",
    "            self.relation_to_id[r] = self.relation_count\n",
    "            self.id_to_relation[self.relation_count] = r\n",
    "            self.relation_count += 1\n",
    "\n",
    "\n",
    "# ==================== TransE ====================\n",
    "class TransE(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, dim):\n",
    "        super().__init__()\n",
    "        self.E = nn.Embedding(num_entities, dim)\n",
    "        self.R = nn.Embedding(num_relations, dim)\n",
    "        nn.init.xavier_uniform_(self.E.weight)\n",
    "        nn.init.xavier_uniform_(self.R.weight)\n",
    "\n",
    "    def forward(self, h, r, t):\n",
    "        return torch.norm(self.E(h) + self.R(r) - self.E(t), p=1, dim=1)\n",
    "\n",
    "    def get_query_embedding(self, h, r):\n",
    "        return self.E(h) + self.R(r)\n",
    "\n",
    "\n",
    "# ==================== TransH （已修复）====================\n",
    "class TransH(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, dim):\n",
    "        super().__init__()\n",
    "        self.E = nn.Embedding(num_entities, dim)\n",
    "        self.R = nn.Embedding(num_relations, dim)      # 关系向量 d_r\n",
    "        self.W = nn.Embedding(num_relations, dim)    # 法向量 W (用于超平面)\n",
    "        nn.init.xavier_uniform_(self.E.weight)\n",
    "        nn.init.xavier_uniform_(self.R.weight)\n",
    "        nn.init.xavier_uniform_(self.W.weight)\n",
    "\n",
    "    def project(self, emb, w):  # 将实体投影到关系超平面上\n",
    "        # w: [B, dim], emb: [B, dim]\n",
    "        norm_w = torch.nn.functional.normalize(w, p=2, dim=1)  # 单位化法向量\n",
    "        scale = torch.sum(emb * norm_w, dim=1, keepdim=True)  # <e, w>\n",
    "        return emb - scale * norm_w  # e - <e, w> * w\n",
    "\n",
    "    def forward(self, h, r, t):\n",
    "        h_emb = self.E(h)  # [B, dim]\n",
    "        t_emb = self.E(t)\n",
    "        r_vec = self.R(r)\n",
    "        W = self.W(r)\n",
    "\n",
    "        h_proj = self.project(h_emb, W)\n",
    "        t_proj = self.project(t_emb, W)\n",
    "\n",
    "        return torch.norm(h_proj + r_vec - t_proj, p=1, dim=1)\n",
    "\n",
    "    def get_query_embedding(self, h, r):\n",
    "        h_emb = self.E(h)\n",
    "        r_vec = self.R(r)\n",
    "        W = self.W(r)\n",
    "        h_proj = self.project(h_emb, W)\n",
    "        return h_proj + r_vec  # 查询向量\n",
    "\n",
    "\n",
    "# ==================== TransD ====================\n",
    "class TransD(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.E = nn.Embedding(num_entities, dim)\n",
    "        self.R = nn.Embedding(num_relations, dim)\n",
    "        self.E_proj = nn.Embedding(num_entities, dim)\n",
    "        self.R_proj = nn.Embedding(num_relations, dim)\n",
    "        nn.init.xavier_uniform_(self.E.weight)\n",
    "        nn.init.xavier_uniform_(self.R.weight)\n",
    "        nn.init.xavier_uniform_(self.E_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.R_proj.weight)\n",
    "\n",
    "    def project(self, e, r_proj):\n",
    "        return e + torch.sum(e * r_proj, dim=1, keepdim=True)\n",
    "\n",
    "    def forward(self, h, r, t):\n",
    "        h_emb = self.project(self.E(h), self.R_proj(r))\n",
    "        t_emb = self.project(self.E(t), self.R_proj(r))\n",
    "        r_vec = self.R(r)\n",
    "        return torch.norm(h_emb + r_vec - t_emb, p=1, dim=1)\n",
    "\n",
    "    def get_query_embedding(self, h, r):\n",
    "        h_emb = self.project(self.E(h), self.R_proj(r))\n",
    "        r_vec = self.R(r)\n",
    "        return h_emb + r_vec\n",
    "\n",
    "\n",
    "# ==================== 训练 & 加载 ====================\n",
    "def train_model(model, model_name, train_dataset, mapper, device):\n",
    "    if os.path.exists(TRAINED_MODEL_PATHS[model_name]):\n",
    "        print(f\"[{model_name}] 已存在训练好的模型，跳过训练\")\n",
    "        return\n",
    "    print(f\"[{model_name}] 开始训练...\")\n",
    "    loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_DECAY_STEP, gamma=LR_DECAY_FACTOR)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        progress = tqdm(loader, desc=f\"{model_name} Epoch {epoch+1}\")\n",
    "        for h_list, r_list, t_list in progress:\n",
    "            h = torch.tensor([mapper.entity_to_id[h] for h in h_list], device=device)\n",
    "            r = torch.tensor([mapper.relation_to_id[r] for r in r_list], device=device)\n",
    "            t = torch.tensor([mapper.entity_to_id[t] for t in t_list], device=device)\n",
    "            ## 【TODO】这里要确保负样本和原来的不一样。\n",
    "            neg_t = torch.randint(0, mapper.entity_count, (len(h), NEGATIVE_SAMPLES), device=device)             \n",
    "\n",
    "            pos_score = model(h, r, t)\n",
    "            neg_score = model(\n",
    "                h.unsqueeze(1).expand(-1, NEGATIVE_SAMPLES).reshape(-1),\n",
    "                r.unsqueeze(1).expand(-1, NEGATIVE_SAMPLES).reshape(-1),\n",
    "                neg_t.reshape(-1)\n",
    "            ).reshape(-1, NEGATIVE_SAMPLES)\n",
    "\n",
    "            loss = torch.mean(torch.relu(pos_score.unsqueeze(1) - neg_score + 1.0))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            progress.set_postfix(loss=loss.item())\n",
    "        scheduler.step()\n",
    "        print(f\"[{model_name}] Epoch {epoch+1} Loss: {epoch_loss / len(loader):.4f}\")\n",
    "\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'entity_count': mapper.entity_count,\n",
    "        'relation_count': mapper.relation_count,\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'entity_to_id': mapper.entity_to_id,\n",
    "        'relation_to_id': mapper.relation_to_id,\n",
    "    }, TRAINED_MODEL_PATHS[model_name])\n",
    "    print(f\"[{model_name}] 模型已保存\")\n",
    "\n",
    "\n",
    "def load_model(model_class, model_path, mapper, device):\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model = model_class(checkpoint['entity_count'], checkpoint['relation_count'], EMBEDDING_DIM)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, dataset, mapper, device, k_list=(1, 3, 10)):\n",
    "    print(\"🔍 开始在开发集上评估模型性能（仅尾实体预测）...\")\n",
    "    model.eval()\n",
    "    hits_at = {k: 0.0 for k in k_list}\n",
    "    mrr = 0.0\n",
    "    count = 0\n",
    "\n",
    "    entity_emb = model.E.weight.data.cpu().numpy()\n",
    "    index = faiss.IndexFlatL2(entity_emb.shape[1])\n",
    "    index.add(entity_emb)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for h, r, t in tqdm(dataset.triples, desc=\"Evaluating\"):\n",
    "            try:\n",
    "                h_id = torch.tensor([mapper.entity_to_id[h]], device=device)\n",
    "                r_id = torch.tensor([mapper.relation_to_id[r]], device=device)\n",
    "                t_id = mapper.entity_to_id[t]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            # 只做尾实体预测 (h, r, ?)\n",
    "            query = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "            _, indices = index.search(query, 1000)\n",
    "            pred_ids = indices[0]\n",
    "\n",
    "            # 过滤\n",
    "            filtered_tails = [tail for head, rel, tail in mapper.all_train_triples if head == h and rel == r and tail != t]\n",
    "            filter_ids = [mapper.entity_to_id[tail] for tail in filtered_tails if tail in mapper.entity_to_id]\n",
    "            for fid in filter_ids:\n",
    "                if fid in pred_ids:\n",
    "                    mask = pred_ids == fid\n",
    "                    pred_ids = np.concatenate([pred_ids[~mask], pred_ids[mask]])\n",
    "\n",
    "            rank = np.where(pred_ids == t_id)[0]\n",
    "            final_rank = rank[0] + 1 if len(rank) > 0 else 10000\n",
    "\n",
    "            for k in k_list:\n",
    "                if final_rank <= k:\n",
    "                    hits_at[k] += 1\n",
    "            mrr += 1.0 / final_rank\n",
    "            count += 1\n",
    "\n",
    "    for k in hits_at:\n",
    "        hits_at[k] /= count\n",
    "    mrr /= count\n",
    "\n",
    "    print(\"✅ 评估完成！\")\n",
    "    print(f\"📊 HITS@1:  {hits_at[1]:.4f}\")\n",
    "    print(f\"📊 HITS@3:  {hits_at[3]:.4f}\")\n",
    "    print(f\"📊 HITS@10: {hits_at[10]:.4f}\")\n",
    "    print(f\"📊 MRR:     {mrr:.4f}\")\n",
    "\n",
    "    return hits_at, mrr\n",
    "\n",
    "\n",
    "# ==================== 融合评估 ====================\n",
    "# def evaluate_ensemble(models_with_weights, dev_dataset, mapper, device, k_list=(1, 3, 10)):\n",
    "#     print(\"\\n\" + \"=\"*50)\n",
    "#     print(\"🚀 开始评估融合模型在开发集上的性能\")\n",
    "#     print(\"=\"*50)\n",
    "\n",
    "#     sample_model = next(iter(models_with_weights.values()))[0]\n",
    "#     entity_emb = sample_model.E.weight.data.cpu().numpy()\n",
    "#     index = faiss.IndexFlatL2(entity_emb.shape[1])\n",
    "#     index.add(entity_emb)\n",
    "\n",
    "#     hits_at = {k: 0.0 for k in k_list}\n",
    "#     mrr = 0.0\n",
    "#     count = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for h, r, t in tqdm(dev_dataset.triples, desc=\"Ensemble Eval\"):\n",
    "#             try:\n",
    "#                 h_id = torch.tensor([mapper.entity_to_id[h]], device=device)\n",
    "#                 r_id = torch.tensor([mapper.relation_to_id[r]], device=device)\n",
    "#                 t_id = mapper.entity_to_id[t]\n",
    "#             except KeyError:\n",
    "#                 continue\n",
    "\n",
    "#             fused_scores = np.zeros(mapper.entity_count)\n",
    "\n",
    "#             for name, (model, weight) in models_with_weights.items():\n",
    "#                 query = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "#                 D, I = index.search(query, 1000)\n",
    "#                 scores = -D[0]\n",
    "#                 for d, idx in zip(scores, I[0]):\n",
    "#                     fused_scores[idx] += weight * d\n",
    "\n",
    "#             # 过滤\n",
    "#             filtered_tails = [tail for head, rel, tail in mapper.all_train_triples if head == h and rel == r and tail != t]\n",
    "#             for tail in filtered_tails:\n",
    "#                 if tail in mapper.entity_to_id:\n",
    "#                     fused_scores[mapper.entity_to_id[tail]] = -1e9\n",
    "\n",
    "#             ranked_idx = np.argsort(fused_scores)[::-1]\n",
    "#             rank = np.where(ranked_idx == t_id)[0]\n",
    "#             final_rank = rank[0] + 1 if len(rank) > 0 else 10000\n",
    "\n",
    "#             for k in k_list:\n",
    "#                 if final_rank <= k:\n",
    "#                     hits_at[k] += 1\n",
    "#             mrr += 1.0 / final_rank\n",
    "#             count += 1\n",
    "\n",
    "#     for k in hits_at:\n",
    "#         hits_at[k] /= count\n",
    "#     mrr /= count\n",
    "\n",
    "#     print(\"✅ 融合模型评估完成！\")\n",
    "#     print(f\"📊 Ensemble HITS@1:  {hits_at[1]:.4f}\")\n",
    "#     print(f\"📊 Ensemble HITS@3:  {hits_at[3]:.4f}\")\n",
    "#     print(f\"📊 Ensemble HITS@10: {hits_at[10]:.4f}\")\n",
    "#     print(f\"📊 Ensemble MRR:     {mrr:.4f}\")\n",
    "\n",
    "#     return hits_at, mrr\n",
    "\n",
    "def evaluate_ensemble(models_with_weights, dev_dataset, mapper, device, k_list=(1, 3, 10)):\n",
    "    \"\"\"\n",
    "    评估融合模型在开发集上的性能（仅尾实体预测）\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🚀 开始评估融合模型在开发集上的性能\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 获取任一模型的实体嵌入，用于 FAISS\n",
    "    sample_model = next(iter(models_with_weights.values()))[0]\n",
    "    entity_emb = sample_model.E.weight.data.cpu().numpy()\n",
    "    index = faiss.IndexFlatL2(entity_emb.shape[1])\n",
    "    index.add(entity_emb)\n",
    "\n",
    "    hits_at = {k: 0.0 for k in k_list}\n",
    "    mrr = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for h, r, t in tqdm(dev_dataset.triples, desc=\"Ensemble Eval\"):\n",
    "            try:\n",
    "                h_id = torch.tensor([mapper.entity_to_id[h]], device=device)\n",
    "                r_id = torch.tensor([mapper.relation_to_id[r]], device=device)\n",
    "                t_id = mapper.entity_to_id[t]\n",
    "            except KeyError:\n",
    "                # 实体不在训练集中，跳过\n",
    "                continue\n",
    "\n",
    "            # ========== 构建融合查询向量 ==========\n",
    "            fused_query = np.zeros((1, EMBEDDING_DIM))\n",
    "            for name, (model, weight) in models_with_weights.items():\n",
    "                q = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "                fused_query += weight * q\n",
    "\n",
    "            # ========== 使用 FAISS 检索 top 1000 候选尾实体 ==========\n",
    "            _, indices = index.search(fused_query, 1000)\n",
    "            candidate_ids = indices[0]  # [1000,]\n",
    "            candidate_scores = -np.linalg.norm(entity_emb[candidate_ids] - fused_query, axis=1)  # 越接近，得分越高\n",
    "\n",
    "            # ========== 强制将真实 tail 加入候选集 ==========\n",
    "            if t_id not in candidate_ids:\n",
    "                # 计算真实 tail 的得分\n",
    "                t_emb = entity_emb[t_id:t_id+1]  # [1, dim]\n",
    "                true_score = -np.linalg.norm(t_emb - fused_query, axis=1).item()\n",
    "                # 替换最后一个候选\n",
    "                candidate_ids = np.append(candidate_ids[:-1], t_id)\n",
    "                candidate_scores = np.append(candidate_scores[:-1], true_score)\n",
    "\n",
    "            # ========== 过滤：排除训练集中已存在的合法三元组（除当前三元组外）==========\n",
    "            filtered_tails = [\n",
    "                tail for head, rel, tail in mapper.all_train_triples\n",
    "                if head == h and rel == r and tail != t\n",
    "            ]\n",
    "            filter_ids = [mapper.entity_to_id[tail] for tail in filtered_tails if tail in mapper.entity_to_id]\n",
    "            # 将过滤掉的实体得分设为极低\n",
    "            for fid in filter_ids:\n",
    "                if fid in candidate_ids:\n",
    "                    pos = np.where(candidate_ids == fid)[0]\n",
    "                    if len(pos) > 0:\n",
    "                        candidate_scores[pos[0]] = -1e9  # 置为负无穷\n",
    "\n",
    "            # ========== 按得分降序排序 ==========\n",
    "            sorted_order = np.argsort(candidate_scores)[::-1]  # 从高到低\n",
    "            ranked_ids = candidate_ids[sorted_order]\n",
    "\n",
    "            # ========== 计算真实 tail 的排名 ==========\n",
    "            rank = np.where(ranked_ids == t_id)[0]\n",
    "            final_rank = rank[0] + 1 if len(rank) > 0 else 10000  # 排名从 1 开始\n",
    "\n",
    "            # ========== 更新指标 ==========\n",
    "            for k in k_list:\n",
    "                if final_rank <= k:\n",
    "                    hits_at[k] += 1\n",
    "            mrr += 1.0 / final_rank\n",
    "            count += 1\n",
    "\n",
    "    # ========== 计算最终指标 ==========\n",
    "    for k in hits_at:\n",
    "        hits_at[k] /= count\n",
    "    mrr /= count\n",
    "\n",
    "    print(\"✅ 融合模型评估完成！\")\n",
    "    print(f\"📊 Ensemble HITS@1:  {hits_at[1]:.4f}\")\n",
    "    print(f\"📊 Ensemble HITS@3:  {hits_at[3]:.4f}\")\n",
    "    print(f\"📊 Ensemble HITS@10: {hits_at[10]:.4f}\")\n",
    "    print(f\"📊 Ensemble MRR:     {mrr:.4f}\")\n",
    "\n",
    "    return hits_at, mrr\n",
    "\n",
    "\n",
    "# ==================== 融合预测 ====================\n",
    "def predict_ensemble(models_with_weights, test_dataset, mapper, device, max_head_entities=None):\n",
    "    print(\"🔍 开始融合预测 (加权融合 + FAISS 加速) ...\")\n",
    "    results = []\n",
    "\n",
    "    sample_model = next(iter(models_with_weights.values()))[0]\n",
    "    entity_emb = sample_model.E.weight.data.cpu().numpy()\n",
    "    index = faiss.IndexFlatL2(entity_emb.shape[1])\n",
    "    index.add(entity_emb)\n",
    "\n",
    "    triples = test_dataset.triples\n",
    "    if max_head_entities:\n",
    "        triples = triples[:max_head_entities]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for h, r, _ in tqdm(triples, desc=\"Ensemble Predict\"):\n",
    "            try:\n",
    "                h_id = torch.tensor([mapper.entity_to_id[h]], device=device)\n",
    "                r_id = torch.tensor([mapper.relation_to_id[r]], device=device)\n",
    "            except KeyError:\n",
    "                preds = [h] * 10\n",
    "                results.append('\\t'.join([h, r] + preds))\n",
    "                continue\n",
    "\n",
    "            fused_query = np.zeros((1, EMBEDDING_DIM))\n",
    "            for name, (model, weight) in models_with_weights.items():\n",
    "                q = model.get_query_embedding(h_id, r_id).detach().cpu().numpy()\n",
    "                fused_query += weight * q\n",
    "\n",
    "            _, indices = index.search(fused_query, 10)\n",
    "            pred_ids = indices[0]\n",
    "            preds = [mapper.id_to_entity[i] for i in pred_ids]\n",
    "            results.append('\\t'.join([h, r] + preds))\n",
    "\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE_PATH), exist_ok=True)\n",
    "    with open(OUTPUT_FILE_PATH, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(results) + '\\n')\n",
    "    print(f\"✅ 融合结果已保存至: {OUTPUT_FILE_PATH}\")\n",
    "\n",
    "    zip_path = OUTPUT_FILE_PATH.replace(\".tsv\", \".zip\")\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        zf.write(OUTPUT_FILE_PATH, arcname=os.path.basename(OUTPUT_FILE_PATH))\n",
    "    print(f\"✅ 已压缩为: {zip_path}\")\n",
    "\n",
    "\n",
    "# ==================== 主函数 ====================\n",
    "def main():\n",
    "    device = torch.device('mps') if torch.backends.mps.is_available() else \\\n",
    "             torch.device('cpu')\n",
    "    print(f\"🚀 使用设备: {device}\")\n",
    "\n",
    "    train_data = KnowledgeGraphDataset(TRAIN_FILE_PATH, max_lines=MAX_LINES, is_train=True)\n",
    "    dev_data = KnowledgeGraphDataset(DEV_FILE_PATH, is_test=False, is_train=False)\n",
    "    test_data = KnowledgeGraphDataset(TEST_FILE_PATH, is_test=True, is_train=False)\n",
    "\n",
    "    mapper = EntityRelationMapper()\n",
    "    mapper.build_mappings(train_data, dev_data, test_data)\n",
    "    print(f\"实体数: {mapper.entity_count}, 关系数: {mapper.relation_count}\")\n",
    "\n",
    "    model_classes = {\n",
    "        'TransE': TransE,\n",
    "        'TransH': TransH,\n",
    "        'TransD': TransD,\n",
    "    }\n",
    "\n",
    "    # 训练所有模型\n",
    "    for name, Cls in model_classes.items():\n",
    "        model = Cls(mapper.entity_count, mapper.relation_count, EMBEDDING_DIM)\n",
    "        train_model(model, name, train_data, mapper, device)\n",
    "\n",
    "    # 加载并评估每个模型\n",
    "    loaded_models_with_weight = {}\n",
    "    for name, Cls in model_classes.items():\n",
    "        model = load_model(Cls, TRAINED_MODEL_PATHS[name], mapper, device)\n",
    "        loaded_models_with_weight[name] = (model, 1.0)\n",
    "        print(f\"\\n📈 正在评估模型: {name}\")\n",
    "        evaluate_model(model, dev_data, mapper, device)\n",
    "\n",
    "    # 🔥 评估融合模型\n",
    "    evaluate_ensemble(loaded_models_with_weight, dev_data, mapper, device)\n",
    "\n",
    "    # 执行融合预测\n",
    "    predict_ensemble(loaded_models_with_weight, test_data, mapper, device, MAX_HEAD_ENTITIES)\n",
    "    print(\"🎉 所有任务完成！融合预测及评估已完成。\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a944170-19af-4473-b120-8b3e3266cb18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896e0dc-2b93-45d2-bd2a-226000ea1223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31994d68-fa0f-460a-b7c3-eebfecfd58c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c6dd2b-bc3c-4368-9239-efdd9f04ca05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a99db-2a74-4d7e-af10-7b59670ad331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d329aab-4099-49b6-a4d1-ed8b637caad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
