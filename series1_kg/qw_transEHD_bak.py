import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import random
import numpy as np
import os
import time
from tqdm import tqdm
import zipfile
from pathlib import Path

# è®¾ç½®éšæœºç§å­
torch.backends.cudnn.deterministic = True
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)

scheme_type = "ensemble"
# æ•°æ®è·¯å¾„ï¼ˆè¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹ï¼‰
BASE_DIR = "/Users/minkexiu/Downloads/GitHub/Tianchi_EcommerceKG_mac"
TRAIN_FILE_PATH = f"{BASE_DIR}/originalData/OpenBG500/OpenBG500_train.tsv"
TEST_FILE_PATH = f"{BASE_DIR}/originalData/OpenBG500/OpenBG500_test.tsv"
DEV_FILE_PATH = f"{BASE_DIR}/originalData/OpenBG500/OpenBG500_dev.tsv"
OUTPUT_FILE_PATH = f"{BASE_DIR}/preprocessedData/OpenBG500_test__{scheme_type}.tsv"

# æ¨¡å‹ä¿å­˜è·¯å¾„
MODEL_DIR = f"{BASE_DIR}/trained_model"
os.makedirs(MODEL_DIR, exist_ok=True)

# æ¨¡å‹è·¯å¾„
TRAINED_MODEL_PATHS = {
    'TransE': f"{MODEL_DIR}/trained_model__transE.pth",
    'TransH': f"{MODEL_DIR}/trained_model__transH.pth",
    'TransD': f"{MODEL_DIR}/trained_model__transD.pth"
}

# è¶…å‚æ•°
EMBEDDING_DIM = 100
LEARNING_RATE = 0.001
WEIGHT_DECAY = 1e-5
EPOCHS = 1
BATCH_SIZE = 256
NEGATIVE_SAMPLES = 10
MAX_LINES = None
MAX_HEAD_ENTITIES = None
LR_DECAY_STEP = 5
LR_DECAY_FACTOR = 0.1
EVAL_CHUNK_SIZE = 4000


# ==================== æ•°æ®é›†å’Œæ˜ å°„ ====================
class KnowledgeGraphDataset(torch.utils.data.Dataset):
    def __init__(self, file_path, is_test=False, max_lines=None):
        self.triples = []
        self._load_data(file_path, is_test, max_lines)
    
    def _load_data(self, file_path, is_test, max_lines):
        print(f"åŠ è½½æ•°æ®: {file_path}")
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            if max_lines:
                lines = lines[:max_lines]
            for line in lines:
                parts = line.strip().split()
                if len(parts) == 3:
                    h, r, t = parts
                    self.triples.append((h, r, t))
                elif is_test and len(parts) == 2:
                    h, r = parts
                    self.triples.append((h, r, "<UNK>"))
        print(f"å…±åŠ è½½ {len(self.triples)} ä¸ªä¸‰å…ƒç»„")

    def __len__(self):
        return len(self.triples)

    def __getitem__(self, idx):
        return self.triples[idx]


def collate_fn(batch):
    h_list, r_list, t_list = zip(*batch)
    return list(h_list), list(r_list), list(t_list)


class EntityRelationMapper:
    def __init__(self):
        self.entity_to_id = {}
        self.id_to_entity = {}
        self.relation_to_id = {}
        self.id_to_relation = {}
        self.entity_count = 0
        self.relation_count = 0

    def build_mappings(self, *datasets):
        entities = set()
        relations = set()
        for dataset in datasets:
            for h, r, t in dataset.triples:
                entities.add(h)
                entities.add(t)
                relations.add(r)
        for e in sorted(entities):
            self.entity_to_id[e] = self.entity_count
            self.id_to_entity[self.entity_count] = e
            self.entity_count += 1
        for r in sorted(relations):
            self.relation_to_id[r] = self.relation_count
            self.id_to_relation[self.relation_count] = r
            self.relation_count += 1


# ==================== TransE ====================
class TransE(nn.Module):
    def __init__(self, num_entities, num_relations, dim):
        super().__init__()
        self.E = nn.Embedding(num_entities, dim)
        self.R = nn.Embedding(num_relations, dim)
        nn.init.xavier_uniform_(self.E.weight)
        nn.init.xavier_uniform_(self.R.weight)

    def forward(self, h, r, t):
        return torch.norm(self.E(h) + self.R(r) - self.E(t), p=1, dim=1)

    def get_score(self, h, r):
        return -torch.norm(self.E(h) + self.R(r).unsqueeze(1) - self.E.weight.unsqueeze(0), p=1, dim=2)


# ==================== TransH ====================
class TransH(nn.Module):
    def __init__(self, num_entities, num_relations, dim):
        super().__init__()
        self.E = nn.Embedding(num_entities, dim)
        self.R = nn.Embedding(num_relations, dim)
        self.W = nn.Embedding(num_relations, dim)  # æ³•å‘é‡
        nn.init.xavier_uniform_(self.E.weight)
        nn.init.xavier_uniform_(self.R.weight)
        nn.init.xavier_uniform_(self.W.weight)

    def project(self, emb, norm):
        norm = torch.nn.functional.normalize(norm, p=2, dim=1)
        return emb - torch.sum(emb * norm, dim=1, keepdim=True) * norm

    def forward(self, h, r, t):
        h_emb = self.project(self.E(h), self.W(r))
        t_emb = self.project(self.E(t), self.W(r))
        return torch.norm(h_emb + self.R(r) - t_emb, p=1, dim=1)

    def get_score(self, h, r):
        h_emb = self.project(self.E(h), self.W(r))
        all_e = self.project(self.E.weight, self.W(r).unsqueeze(1))
        return -torch.norm(h_emb.unsqueeze(1) + self.R(r).unsqueeze(1) - all_e, p=1, dim=2)


# ==================== TransD ====================
class TransD(nn.Module):
    def __init__(self, num_entities, num_relations, dim):
        super().__init__()
        self.dim = dim
        self.E = nn.Embedding(num_entities, dim)
        self.R = nn.Embedding(num_relations, dim)
        self.E_proj = nn.Embedding(num_entities, dim)
        self.R_proj = nn.Embedding(num_relations, dim)
        nn.init.xavier_uniform_(self.E.weight)
        nn.init.xavier_uniform_(self.R.weight)
        nn.init.xavier_uniform_(self.E_proj.weight)
        nn.init.xavier_uniform_(self.R_proj.weight)

    def project(self, e, r_proj):
        return e + torch.sum(e * r_proj, dim=1, keepdim=True)  # å¤–ç§¯æŠ•å½±

    def forward(self, h, r, t):
        h_emb = self.project(self.E(h), self.R_proj(r))
        t_emb = self.project(self.E(t), self.R_proj(r))
        r_vec = self.R(r)
        return torch.norm(h_emb + r_vec - t_emb, p=1, dim=1)

    def get_score(self, h, r):
        h_emb = self.project(self.E(h), self.R_proj(r))
        all_e = self.E.weight
        all_e_proj = all_e + torch.sum(all_e * self.R_proj(r).unsqueeze(1), dim=2, keepdim=True)
        r_vec = self.R(r)
        return -torch.norm(h_emb.unsqueeze(1) + r_vec.unsqueeze(1) - all_e_proj, p=1, dim=2)


# ==================== è®­ç»ƒå•ä¸ªæ¨¡å‹ ====================
def train_model(model, model_name, train_dataset, mapper, device):
    if os.path.exists(TRAINED_MODEL_PATHS[model_name]):
        print(f"[{model_name}] å·²å­˜åœ¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè·³è¿‡è®­ç»ƒ")
        return

    print(f"[{model_name}] å¼€å§‹è®­ç»ƒ...")
    loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_DECAY_STEP, gamma=LR_DECAY_FACTOR)

    model.to(device)
    model.train()

    for epoch in range(EPOCHS):
        epoch_loss = 0
        progress = tqdm(loader, desc=f"{model_name} Epoch {epoch+1}")
        for h_list, r_list, t_list in progress:
            h = torch.tensor([mapper.entity_to_id[h] for h in h_list], device=device)
            r = torch.tensor([mapper.relation_to_id[r] for r in r_list], device=device)
            t = torch.tensor([mapper.entity_to_id[t] for t in t_list], device=device)

            neg_t = torch.randint(0, mapper.entity_count, (len(h), NEGATIVE_SAMPLES), device=device)
            pos_score = model(h, r, t)
            neg_score = model(h.unsqueeze(1).expand(-1, NEGATIVE_SAMPLES).contiguous().view(-1),
                              r.unsqueeze(1).expand(-1, NEGATIVE_SAMPLES).contiguous().view(-1),
                              neg_t.view(-1)).view(-1, NEGATIVE_SAMPLES)

            loss = torch.mean(torch.relu(pos_score.unsqueeze(1) - neg_score + 1.0))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
            progress.set_postfix(loss=loss.item())
        scheduler.step()
        print(f"[{model_name}] Epoch {epoch+1} Loss: {epoch_loss / len(loader):.4f}")

    # ä¿å­˜æ¨¡å‹
    torch.save({
        'model_state_dict': model.state_dict(),
        'entity_count': mapper.entity_count,
        'relation_count': mapper.relation_count,
        'embedding_dim': EMBEDDING_DIM,
        'entity_to_id': mapper.entity_to_id,
        'relation_to_id': mapper.relation_to_id,
    }, TRAINED_MODEL_PATHS[model_name])
    print(f"[{model_name}] æ¨¡å‹å·²ä¿å­˜")


# ==================== åŠ è½½æ¨¡å‹ ====================
def load_model(model_class, model_path, mapper, device):
    checkpoint = torch.load(model_path, map_location=device)
    model = model_class(checkpoint['entity_count'], checkpoint['relation_count'], EMBEDDING_DIM)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    return model


# ==================== èåˆé¢„æµ‹ï¼ˆä¿®æ­£ç‰ˆï¼‰====================
def predict_ensemble(models_with_weights, test_dataset, mapper, device, max_head_entities=None):
    """
    models_with_weights: dict, key: name, value: (model, weight)
    """
    print("ğŸ” å¼€å§‹èåˆé¢„æµ‹ (åŠ æƒèåˆ: TransE + TransH + TransD) ...")

    results = []
    triples = test_dataset.triples
    if max_head_entities:
        triples = triples[:max_head_entities]

    with torch.no_grad():
        for h, r, _ in tqdm(triples, desc="Ensemble Predict"):
            try:
                h_id = torch.tensor([mapper.entity_to_id[h]], device=device)
                r_id = torch.tensor([mapper.relation_to_id[r]], device=device)
            except KeyError as e:
                print(f"âš ï¸ å®ä½“æˆ–å…³ç³»æœªåœ¨è®­ç»ƒé›†ä¸­å‡ºç°: {e}")
                preds = [h] * 10  # å›é€€
                results.append('\t'.join([h, r] + preds))
                continue

            # å­˜å‚¨æ¯ä¸ªæ¨¡å‹çš„å¾—åˆ†ï¼ˆå¯¹æ‰€æœ‰å®ä½“ï¼‰
            all_scores = torch.zeros(mapper.entity_count, device=device)

            for name, (model, weight) in models_with_weights.items():
                # è·å–æ‰€æœ‰å®ä½“çš„åµŒå…¥: [E]
                entity_emb = model.E.weight  # [E, dim]

                # è·å–æŸ¥è¯¢å‘é‡: [1, dim]
                if hasattr(model, 'get_query_embedding'):
                    query = model.get_query_embedding(h_id, r_id)  # è¿”å› numpy æˆ– tensor
                    if isinstance(query, np.ndarray):
                        query = torch.tensor(query, device=device)
                else:
                    # é»˜è®¤ TransE é£æ ¼
                    query = model.E(h_id) + model.R(r_id)  # [1, dim]

                # è®¡ç®—è·ç¦»ï¼ˆL1ï¼‰
                # [1, dim] vs [E, dim] -> [E]
                scores = -torch.norm(query - entity_emb, p=1, dim=1)  # è´Ÿè·ç¦»ä½œä¸ºå¾—åˆ†
                all_scores += weight * scores

            # Top-10
            _, topk_indices = torch.topk(all_scores, k=10)
            preds = [mapper.id_to_entity[i.item()] for i in topk_indices]
            results.append('\t'.join([h, r] + preds))

    # ä¿å­˜ç»“æœ
    os.makedirs(os.path.dirname(OUTPUT_FILE_PATH), exist_ok=True)
    with open(OUTPUT_FILE_PATH, 'w', encoding='utf-8') as f:
        f.write('\n'.join(results) + '\n')
    print(f"âœ… èåˆç»“æœå·²ä¿å­˜è‡³: {OUTPUT_FILE_PATH}")

    zip_path = OUTPUT_FILE_PATH.replace(".tsv", ".zip")
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
        zf.write(OUTPUT_FILE_PATH, arcname=os.path.basename(OUTPUT_FILE_PATH))
    print(f"âœ… å·²å‹ç¼©ä¸º: {zip_path}")


# ==================== ä¸»å‡½æ•°ï¼ˆä¿®æ­£ç‰ˆï¼‰====================
def main():
    device = torch.device('mps') if torch.backends.mps.is_available() else \
             torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")

    # åŠ è½½æ•°æ®
    train_data = KnowledgeGraphDataset(TRAIN_FILE_PATH, max_lines=MAX_LINES)
    test_data = KnowledgeGraphDataset(TEST_FILE_PATH, is_test=True)
    dev_data = KnowledgeGraphDataset(DEV_FILE_PATH)

    mapper = EntityRelationMapper()
    mapper.build_mappings(train_data, test_data, dev_data)
    print(f"å®ä½“æ•°: {mapper.entity_count}, å…³ç³»æ•°: {mapper.relation_count}")

    # å®šä¹‰æ¨¡å‹
    model_classes = {
        'TransE': TransE,
        'TransH': TransH,
        'TransD': TransD,
    }

    # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
    for name, Cls in model_classes.items():
        model = Cls(mapper.entity_count, mapper.relation_count, EMBEDDING_DIM)
        train_model(model, name, train_data, mapper, device)

    # âœ… åŠ è½½æ‰€æœ‰æ¨¡å‹ï¼Œå¹¶åŒ…è£…æˆ (model, weight)
    loaded_models_with_weight = {}
    for name, Cls in model_classes.items():
        model = load_model(Cls, TRAINED_MODEL_PATHS[name], mapper, device)
        # è®¾ç½®é»˜è®¤æƒé‡ï¼ˆå¯åç»­è°ƒä¼˜ï¼‰
        weight = 1.0
        if name == 'ConvE':
            weight = 1.2  # å‡è®¾ ConvE æ›´å¼º
        loaded_models_with_weight[name] = (model, weight)
        print(f"[{name}] æ¨¡å‹å·²åŠ è½½ï¼Œæƒé‡: {weight}")

    # âœ… è°ƒç”¨èåˆé¢„æµ‹
    predict_ensemble(loaded_models_with_weight, test_data, mapper, device, MAX_HEAD_ENTITIES)

    print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼èåˆé¢„æµ‹å·²ç”Ÿæˆã€‚")


if __name__ == "__main__":
    main()